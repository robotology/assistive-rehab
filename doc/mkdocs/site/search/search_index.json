{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Assistive-rehab","text":"<p>Assistive-rehab is a framework for developing the assistive intelligence of R1 robot for clinical rehabilitation and tests. The project is being developed within the Joint Lab between IIT and Fondazione Don Carlo Gnocchi Onlus.</p>"},{"location":"#scenarios","title":"Scenarios","text":"<p>A humanoid platform offers the potential of providing objective and quantitative measurements, while delivering repetitive therapy with constant monitoring. Motivated by this rationale, with the experts of Fondazione Don Gnocchi, we identified two main scenarios:</p> <ul> <li>rehabilitation: R1 guides a patient in performing physical exercises for the upper limbs and the torso. The robot physically shows the specific exercise to perform, while verifying the correct execution and providing a verbal feedback in real-time:</li> </ul> <p></p> <p>The Train with Me study</p> <p>This scenario was adopted to compare the users' engagement and movement performance when exercising with a real robot and its virtual counterpart. Results show that both the levels of engagement and the participants\u2019 performance are higher with the real robot than with the virtual one! Check it out in our paper.</p> <ul> <li>clinical tests: R1 explains the clinical test to the patient, by physically pointing and reaching keypoints in the environment, while verifying if the test is passed and extracting real-time metrics. The developed test is the Timed Up and Go (TUG), with the patient having to stand up from a chair, walk for 3 meters, turn, go back to the chair and sit down:</li> </ul> <p></p>"},{"location":"#rehabilitation-components","title":"Rehabilitation: components","text":"<p>The rehabilitation scenario has been developed through the following components:</p> <p></p> <ul> <li>R1 is equipped with an Intel RealSense D435 depth camera, which provides RGB images along with depth data;</li> <li>acquisition and storage are carried out by <code>skeletonRetriever</code> and <code>objectsPropertiesCollector</code> respectively. <code>skeletonRetriever</code> merges 2D skeleton data, as acquired by <code>yarpOpenPose</code>, along with the depth information provided by the depth sensor. 3D skeletons are stored in the OPC. This architecture also allows to replay experiments, using the combination of <code>yarpdatadumper</code> and <code>yarpdataplayer</code>, giving to physiotherapists the possibility of computing new metrics (not run online) on the replayed experiment, and of maintaining a logbook of all the patient's exercises;</li> <li>the face recognition pipeline is responsible for associating a labeled face to a skeleton, which thus becomes unambiguously tagged. This allows potentially to personalize exercises according to the specific rehabilitation path of a patient;</li> <li>the analysis of motion extracts in real-time salient metrics, as the Range of Motion (RoM) of the articular angles (for abduction and internal/external rotation), and the speed of the end-point (for a reaching task). The module also exports relevant data for enabling offline reporting of the experiments at the end of the session. The motion analysis provides a useful tool to physiotherapists for both evaluating the quality of movement of a patient and benefiting from the offline report as required documentation to produce after a session;</li> <li>the final feedback is produced by means of an action recognizer and a feedback analysis. The action recognizer uses 2D keypoints trajectories to predict the label of the performed exercise and triggers the analysis only if the predicted label is the same as the exercise to perform. If so, the analysis compares the observed skeleton with a (predefined) template skeleton to produce real-time feedback on the range of motion, the speed and how well the target is reached;</li> <li>the robot arms' movement is controlled through <code>ctpService</code>;</li> <li>the <code>interactionManager</code> manages the interaction between all the modules involved.</li> </ul>"},{"location":"#clinical-tests-components","title":"Clinical tests: components","text":"<p>The clinical test scenario consists of the following components:</p> <p></p> <p>We re-used some of the components developed for the rehabilitation scenario, specifically the acquisition and storage and the motion analysis, which was extended in order to cover the lower limbs and to extract metrics as the step length and width, the walking speed and the number of steps. Additionally, we developed new components in order to enrich the human-robot interaction and the robot's perception:</p> <ul> <li>a reactive navigation system, which allows the robot to navigate based on the received perceptual stimuli: the robot can reach fixed points in the environment (such as the start and the finish lines) and follow the user maintaining a fixed distance along a straight path. In such a system, the environment is supposed to be free from obstacles;</li> <li>a speech recognition and interpretation pipeline, which relies on the Google services API. The pipeline receives the sound from an external microphone and 1. retrieves the speech transcript from Google Speech cloud services and 2. analyses such transcript to retrieve the sentence structure and meaning, relying on Google Language Cloud services. Such system provides a flexible and natural interaction, given by the capability to interpret the question, rather than simply recognize it.;</li> <li>a speech trigger system based on the use of a Mystrom wifi button: when pressed, the button is configured to produce a http request handled by means of a <code>node.js</code>, which triggers the speech pipeline;</li> <li>a visual line detection, which detects start and finish lines composed of specific ArUco markers placed on the floor. Such lines indicate the start and the end of the path to the patient, while being fixed reference points for the robot;</li> <li>the robot points to the reference lines using its arms through <code>ctpService</code>;</li> <li>the <code>managerTUG</code> manages the interaction between all the modules involved.</li> </ul>"},{"location":"TUG/","title":"TUG","text":"<p>This application has been developed during 2019 with the aim of expanding the handled scenarios to clinical test beyond rehabilitation.</p> <p>The Timed Up and Go (TUG) measures the time the patient takes to stand up from a chair, walk for 3 meters and cross a line on the floor, turn, go back to the chair and sit down, as shown in the following picture:</p> <p></p> <p>Note</p> <p>This was realized within the simulation environment <code>gazebo</code>.</p> <p>The application includes the following steps:</p> <ol> <li>R1 engages the user, proposing a clinical test of the lower limbs, specifically the TUG;</li> <li>R1 explains the test, navigating the environment to reach the lines on the floor and pointing at them;</li> <li>R1 analyzes in real-time the correct execution of the test, verifying that the user crosses the finish line and goes back to the chair and simultaneously extracting real-time walking metrics (step length and width, walking speed and number of steps);</li> <li>R1 eventually replies to questions during the test explanation and / or execution, if the user presses the provided button;</li> <li>R1 produces a final report of the test, which includes the evaluated walking metrics.</li> </ol> <p>Note</p> <p>The code implementing this application is tagged as v0.5.0.</p>"},{"location":"Y1M5/","title":"Y1M5","text":"<p>This application has been developed during the first semester of 2018 with the aim of starting implementing the features required to support R1 as artificial assistant of the physiotherapist. The fundamental aspects considered are the following:</p> <ol> <li>R1 engages the user, proposing a rehabilitative exercise of the upper limbs, chosen from a predefined repertoire;</li> <li>the exercise the user has to perform is shown on the screen, through a template skeleton;</li> <li>R1 analyzes in real-time the execution of the exercise, providing a verbal feedback limited to three levels of quality of the movement;</li> <li>the analyzed metric is updated in real-time and shown on the screen;</li> <li>R1 processes a final report of the exercise which includes the information acquired during the interaction;</li> <li>R1 keeps engaging a user, proposing movements from the repertoire which have not been proposed before, by using face recognition and stored data related to past interactions;</li> <li>the physical interaction between R1 and the patient is limited to the motion of R1's head.</li> </ol> <p>Note</p> <p>The code implementing this application is tagged as v0.1.0.</p>"},{"location":"Y1Q3/","title":"Y1Q3","text":"<p>This application has been developed during the second semester of 2018, with the aim of significantly improving Y1M5 by considering the following aspects:</p> <ol> <li> <p>increasing the motor functioning of R1: R1 physically shows to the patient the movement to replicate, removing the need of visualizing the movement on the screen. This improvement is intended to support the interaction between robot and patient, promoting the use of a robotic platform rather than a simple fixed camera, with the final aim of improving the quality of rehabilitation of a patient;  </p> </li> <li> <p>extending the motion analysis to the end-point: the trajectory of the end-point (hand) is evaluated during a reaching task, including also the speed profile;</p> </li> <li> <p>extending the provided feedback: the feedback includes important properties of the movement, such as the speed and the range of motion, improving significantly the capability of correction.</p> </li> </ol> <p>Including the mentioned point, the application considers the following steps:</p> <ol> <li>R1 engages the user, proposing a rehabilitative exercise of the upper limbs;</li> <li>R1 physically shows the exercise the user has to perform from a predefined repertoire:</li> </ol> Abduction Internal rotation External rotation <p> </p> <p> </p> <p> </p> <ol> <li>R1 analyzes in real-time the execution of the exercise, providing an extended verbal feedback which includes the speed of the movement, the range of motion and how well the target has been reached;</li> <li>the analyzed metric is updated in real-time and shown on the screen;</li> <li>R1 processes a final report of the exercise which includes the information acquired during the interaction;</li> <li>R1 keeps engaging a user, proposing movements from the repertoire which have not been proposed before, by using face recognition and stored data related to past interactions;</li> </ol> <p>Note</p> <p>The code implementing this application is tagged as v0.2.0.</p>"},{"location":"Y2Q2/","title":"Y2Q2","text":"<p>This application has been developed during the first semester of 2019, with the aim of enlarging the scenarios to include clinical tests (besides rehabilitation exercises as in Y1Q3). Such scenarios highlight the advantage of using a robot, given by the autonomous execution of the test (which is highly repetitive for physiotherapists) and the storage of quantitative results.</p> <p>We started with the Timed Up and Go (TUG), which measures the time the patient takes to get up from a chair, walk for 3 meters, turn around, go back to the chair and seat again. While it's easy for a physiotherapist to measure the execution time, it's much more complicated to evaluate limbs metrics during the test (such as range of motion, step length), unless of equipping the patient and the room with sensors.</p> <p>In such scenario, the robot has to:</p> <ul> <li>explain the test to the patient;</li> <li>naturally interact with the patient and reply to his / her questions;</li> <li>navigate in the environment;</li> <li>monitor the patient while doing the test;</li> <li>extract real-time metrics of the lower limbs and export them in a final report for further analysis.  </li> </ul>"},{"location":"about/","title":"About","text":"<p><code>assistive-rehab</code> has been developed within the Joint Lab between IIT and Fondazione Don Carlo Gnocchi Onlus.</p>"},{"location":"about/#contributors","title":"Contributors","text":"alphabetical order main components Ettore Landini (@elandini84) Testing &amp; Debugging Mattia Fussi (@mfussi66) Testing &amp; Debugging Marco Randazzo (@randaz81) Testing &amp; Debugging Stefano Bernagozzi (@ste93) Testing &amp; Debugging Ugo Pattacini (@pattacini) Skeletons Handling Vadim Tikhanoff (@vtikha) Face Recognition, I/F to OpenPose Valentina Vasco (@vvasco) Action Recognition, Movements Analysis"},{"location":"about/#special-thanks","title":"Special Thanks","text":"alphabetical order reason Diego Ferigo (@diegoferigo) Hints on the website"},{"location":"align_signals/","title":"How to temporally align two signals","text":"<p>Some applications require signals to be temporally aligned. For example, when analyzing and comparing the current and the template skeleton for producing a feedback, joints must be aligned.</p> <p>Given two 1D signals, whose temporal samples are stored in vectors v1 and v2, you can use the following code snippet to get the aligned versions w1 and w2:  </p> <pre><code>assistive_rehab::Dtw dtw(-1);\nstd::vector&lt;double&gt; w_v1,w_v2;\ndtw.align(v1,v2,w_v1,w_v2);\ndouble d = dtw.getDistance();\n</code></pre> <p>Tip</p> <p>In the example, no adjustment window condition is applied. Therefore the search of the warping path is done along the whole distance matrix. To limit the search, you can create the obejct <code>dtw</code> by passing the desired window, e.g. <code>assistive_rehab::Dtw dtw(8)</code>.</p> <p>The following images show two signals before and after the application of DTW:</p> <p>Before DTW</p> <p></p> <p>After DTW</p> <p> </p> <p>For the multidimensional case, the code can be adapted as following:</p> <pre><code>assistive_rehab::Dtw dtw(-1);\nstd::vector&lt;std::vector&lt;double&gt;&gt; w_v1,w_v2;\ndtw.align(v1,v2,w_v1,w_v2);\ndouble d = dtw.getDistance();\n</code></pre> <p>Note</p> <p>v1 and v2 are defined as <code>std::vector&lt;std::vector&gt;</code>.</p>"},{"location":"comparison_releases/","title":"Comparison between Y1Q3 and Y1M5","text":""},{"location":"comparison_releases/#acquisition","title":"Acquisition","text":"<p>The improvements that Y1Q3 introduces in terms of acquisition are listed below:</p> <ol> <li>Filtering the disparity map;</li> <li>Optimizing the skeleton;</li> <li>Reducing latencies.</li> </ol>"},{"location":"comparison_releases/#filtering-the-disparity-map","title":"Filtering the disparity map","text":"<p>The disparity map provided by the camera is inaccurate around the human contour and might have holes, leading keypoints close to the contour (e.g. hands) or on an hole to be projected incorrectly. For example, the following video shows the depth map for an abduction movement, where the hand is projected to infinite:</p> <p> </p> <p>In Y1M5, the disparity map is used as provided by the camera, without any additional processing, and thus it is affected by the effect described. In Y1Q3, the disparity map is eroded (and thus the depth map dilated), which has the double effect of:</p> <ol> <li>increasing the probability for keypoints closer to the contour to fall within the correct depth map;</li> <li>filling holes in the depth.</li> </ol> <p>The following video shows the effect of filtering the depth map and keypoints falling within the correct depth:</p> Unprocessed depth (Y1M5) Filtered depth (Y1Q3) Keypoints inside the depth <p> </p> <p> </p> <p> </p>"},{"location":"comparison_releases/#optimizing-the-skeleton","title":"Optimizing the skeleton","text":"<p>Exercises that require movements parallel to the optical axis make some keypoints ambiguous. For example, in the external and internal rotation, the elbow is not directly observable as the hand occludes it. Therefore both keypoints are projected to the same depth, as shown here:</p> Without optimization (Y1M5) <p> </p> <p>In Y1Q3, we introduce an optimization of the skeleton, which adjusts the depth of the keypoints such that the length of the arms is equal to that observed during an initial phase. The following image shows the result of the optimization, in which elbow and hand are projected correctly.</p> With optimization (Y1Q3) <p> </p>"},{"location":"comparison_releases/#reducing-latencies","title":"Reducing latencies","text":"<p><code>yarpOpenPose</code> introduces a visible latency in the depth map, as shown here:</p> Without <code>yarpOpenPose</code> With <code>yarpOpenPose</code> <p> </p> <p> </p> <p>In Y1Q3, <code>yarpOpenPose</code> propagates the depth image in sync with the output of skeleton detection, in order to equalize the delay between the two streams.</p>"},{"location":"comparison_releases/#extending-the-feedback","title":"Extending the feedback","text":""},{"location":"comparison_releases/#y1m5","title":"Y1M5","text":"<p>In Y1M5, the feedback is based on the definition of dynamic joints (i.e. joints performing the exercise (1)) and static joints (i.e. joints staying in the initial position (2)):</p> <ol> <li>dynamic joints contribute to the dynamic score, which computes their range of motion in a predefined temporal window, by awarding joints moving within the correct range of motion;</li> <li>static joints contribute to the static score, which computes their deviation from a predefined resting position.</li> </ol> <p>Dynamic and static scores are combined to produce three levels:</p> <ol> <li> <p>low: both dynamic and static scores are low, producing the following feedback:</p> <p>\"You are not moving very well!\"</p> </li> <li> <p>medium: either dynamic or static score is medium, producing the following feedback:</p> <p>\"You are doing the exercise correctly, but you could do it better.\"</p> </li> <li> <p>high: both dynamic and static scores are high, producing the following feedback:</p> <p>\"You are moving very well!\"</p> </li> </ol> <p>Failure</p> <ul> <li>the feedback does not provide a real correction of the movement;</li> <li>the speed of the movement is not directly taken into account;</li> <li>the feedback can be positive even when a wrong movement is performed (for example, a different movement with the same moving joints as external rotation instead of internal and viceversa, or a random movement with dynamic joints moving within the range of motion and static joints not deviating enough from the resting position).</li> </ul>"},{"location":"comparison_releases/#y1q3","title":"Y1Q3","text":"<p>To overcome the drawbacks of Y1M5, in Y1Q3 the concept of dynamic and static joints is removed and the exercise is treated in its entirety as an action. Therefore the feedback is produced based on the following two layers:</p> <ol> <li>Action recognition: which classifies the exercise and triggers the second layer if the predicted class equals the label of the exercise to perform. Otherwise the feedback produced is negative. This layer is fundamental as it guarantees a random or a wrong movement to not produce a positive feedback.</li> <li>Joint analysis: which compares the observed skeleton with a predefined template for that movement, differently based on the exercise. The comparison with a template skeleton allows us to analyze also the speed of the movement.</li> </ol> <p>With this architecture, the feedback is extended and articulated in the following levels:</p> <ul> <li> <p>positive feedback:</p> <p>\"You are moving very well!\"</p> </li> <li> <p>feedback for range of motion exercises:</p> <ol> <li> <p>feedback on the speed:</p> <p>\"Move the arm faster/slower!\"</p> </li> <li> <p>feedback on the range of motion:</p> <p>\"Move the arm further up/down!\"</p> </li> </ol> </li> <li> <p>feedback for reaching exercises:</p> <p>\"You are not reaching the target!\"</p> </li> <li> <p>negative feedback:</p> <p>\"You are doing the wrong exercise. Please, repeat the movements I show you.\"</p> </li> </ul>"},{"location":"comparison_releases/#action-recognition","title":"Action Recognition","text":"<p>The action recognition is carried out using a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells.</p> <ul> <li> <p>Input to the network: 2D joints of the upper body from the <code>skeletonRetriever</code>. The output of the <code>skeletonRetriever</code> is preferable to that of <code>yarpOpenPose</code>, as the <code>skeletonRetriever</code> unambiguously identifies a skeleton by tag, avoiding ambiguities due to the presence of several skeletons.</p> </li> <li> <p>Preprocessing: The following operations are applied to the skeleton:</p> <ol> <li>normalization, such that the length of each segment is 1. This prevents having different results for different physiques;</li> <li>rototranslation, derived by the first observed skeleton;</li> <li>scale by a factor of 10, in order to have values between -1.0,1.0.</li> </ol> </li> <li> <p>Training set: 1 subject performs the following 6 exercises (i.e. 6 classes):</p> <ol> <li><code>abduction-left</code>;</li> <li><code>internal-rotation-left</code>;</li> <li><code>external-rotation-left</code>;</li> <li><code>reaching-left</code>;</li> <li><code>static</code> (the subject remains steady);</li> <li><code>random</code> (the subject moves randomly).</li> </ol> <p>The first 3 movements are repeated 10 times and the full exercise repeated 5 times. For the 4<sup>th</sup> movement, 4 targets to be reached are defined, distributed on the corners/center of a square, centered around the <code>shoulderLeft</code> of the subject. Each dataset was recorded from a frontal and a side view and can be found at this link. Parameters used for training are the following:</p> <ol> <li><code>n_hidden</code> = 22</li> <li><code>n_steps</code> (temporal window) = 30</li> <li><code>learning_rate</code> = 0.0005</li> <li><code>batch_size</code> = 256</li> <li><code>epochs</code> = 400</li> </ol> </li> <li> <p>Validation set: The same subject, but previously unseen data, were used for testing the network.</p> </li> <li> <p>Accuracy: We get an accuracy of 92.2% with the following confusion matrix:</p> </li> </ul> <p> </p>"},{"location":"comparison_releases/#joint-analysis","title":"Joint analysis","text":"<p>This analysis is differentiated according to the exercises, which are classified as:</p> <ol> <li>range of motion exercises (i.e. abduction, internal and external rotation);</li> <li>reaching exercises.</li> </ol>"},{"location":"comparison_releases/#range-of-motion-exercises","title":"Range of Motion exercises","text":"<p>The joints under analysis for these movements are: <code>elbowLeft</code>, <code>handLeft</code>. These movements can produce two feedbacks, i.e. on the speed and on the range of motion. The feedback is provided according to a predefined hierarchy which prioritizes the speed, followed by the range of motion. Therefore, a positive feedback is produced only when both checks are fine.</p> <ol> <li>Speed: Fourier analysis</li> </ol> <p>We perform the Fourier transform of each component of the joints under analysis in a predefined temporal window, for both the observed and the template skeleton. The difference in frequency is computed as <code>df = f_skeleton - f_template</code> and thus we can have two possible cases:</p> <ol> <li><code>df &gt; 0</code> =&gt; feedback: \"Move the arm faster\"</li> <li> <p><code>df &lt; 0</code> =&gt; feedback: \"Move the arm slower\"</p> </li> <li> <p>Range of motion: Dynamic Time Warping (DTW) plus error statistical analysis</p> </li> </ol> <p>The DTW is applied to each component of the joints under analysis, for both the observed and the template skeleton, allowing us to temporally align the signals to compare. Once joints are aligned, the error between the observed and template joints under analysis is computed.        A statistical analysis is carried out, which looks for tails in the error distribution. Tails can be identified using the skewness of the distribution. Three cases can be identified:</p> <ol> <li>the skeleton and the template are moving similarly. Therefore the error in position is close to 0, generating a distribution centered around 0 (i.e. with low skewness):</li> <li>the skeleton has an higher range of motion than the template. Therefore the error will be positive, generating a distribution with a positive tail (i.e. with a positive skewness):</li> <li>the skeleton has a lower range of motion than the template. Therefore the error will be negative, generating a distribution with a negative tail (i.e. with a negative skewness):</li> </ol>"},{"location":"comparison_releases/#reaching","title":"Reaching","text":"<p>The joint under analysis for this movement is <code>handLeft</code>. This movement produces a feedback related to how well a predefined target is reached.</p> <ol> <li>Statistical analysis of the reached target</li> </ol> <p>We define a sphere around the predefined target and consider the points of the template which fall within the sphere. Statistics of the distribution of the template points are extracted to describe how the points should be distributed around the target. We then compute the number of points of the observed skeleton which fall within the template distribution. If the number of inliers is above a predefined threshold, the target is considered reached and a positive feedback is produced, otherwise the target is considered not reached.</p>"},{"location":"comparison_releases/#comparison","title":"Comparison","text":"<p>The following tables compare feedbacks produced by the two pipelines developed in Y1M5 and in Y1Q3 respectively, in response to the same movement. Corrrect feedbacks are highlighted.</p> <ul> <li><code>abduction-left</code></li> </ul> Y1M5 Y1Q3 Correct 1. You are moving very well! 2. You are moving very well! 1. You are moving very well! 2. You are moving very well! Fast 1.You are moving very well! 2.You are moving very well! 1. Move the left arm slower! 2. Move the left arm slower! Slow 1.You are moving very well! 2.You are moving very well! 1. Move the left arm further up!  2. Move the left arm further up! Low ROM 1.You are moving very well! 2.You are moving very well! 1. Move the left arm further up! 2. Move the left arm further up! Wrong 1. You are doing the exercise correctly, but you could do it better.  2. You are doing the exercise correctly, but you could do it better. 1. You are doing the wrong exercise. 2. You are doing the wrong exercise. <ul> <li><code>external-rotation-left</code></li> </ul> Y1M5 Y1Q3 Correct 1. You are doing the exercise correctly, but you could do it better.  2. You are doing the exercise correctly, but you could do it better. 1. Move the left arm slower!  2.You are moving very well! Fast 1.You are moving very well!  2.You are moving very well! 1.Move the left arm slower! 2.Move the left arm slower! Slow 1. You are doing the exercise correctly, but you could do it better.  2. You are moving very well! 1. You are moving very well!  2. Move the left arm faster! Low ROM 1.You are moving very well!  2. You are doing the exercise correctly, but you could do it better. 1.You are moving very well!  2.Move the left arm slower! Wrong 1. You are doing the exercise correctly, but you could do it better.  2. You are doing the exercise correctly, but you could do it better. 1. You are doing the wrong exercise. 2. You are doing the wrong exercise. <ul> <li><code>internal-rotation-left</code></li> </ul> Y1M5 Y1Q3 Correct 1.You are moving very well! 2.You are moving very well! 1.You are moving very well! 2.You are moving very well! Fast 1.You are moving very well!  2.You are moving very well! 1.Move the left arm slower! 2.Move the left arm slower! Slow 1.You are moving very well!  2.You are moving very well! 1.You are moving very well!  2.Move the left arm faster! Low ROM 1.You are not moving very well! 2.You are not moving very well! 1. Move the left arm backwards! 2. Move the left arm backwards! Wrong 1.You are not moving very well! 2.You are not moving very well! 1. You are doing the wrong exercise. 2. You are doing the wrong exercise. <p>Success</p> <ul> <li>in Y1Q3 the correct feedback is provided for fast movements, while in Y1M5 the feedback is always positive, regardless the speed;</li> <li>in Y1Q3 the correct feedback is provided for low ROM movements, while in Y1M5 the feedback is always positive (except for the external rotation, which generates a medium score since there are time instants where the range of motion is below the minimum one);</li> <li>in Y1Q3 a negative feedback is provided for wrong movements, while in Y1M5 the feedback is generated by a medium score, (except for the internal rotation, due to the static score, which is low as static joints deviate a lot from the resting position).</li> </ul> <p>Failure</p> <ul> <li>in Y1Q3 a positive feedback is provided for slow movements. This occurs as the template skeleton moves slowly to emulate the movement shown by the robot and the feedback on the speed is triggered only by a very slow execution of the exercise;</li> <li>in Y1Q3 a feedback on the speed is provided for an external rotation with low ROM, as the threshold on the frequency is set low to trigger also a feedback on slow movements.</li> </ul>"},{"location":"create_new_skeleton/","title":"How to manage a skeleton object","text":"<p>This tutorial will explain you the main functionalities to manage a skeleton object for:</p> <ul> <li>defining a skeleton;</li> <li>retrieving a skeleton;</li> <li>accessing single keypoints of a skeleton;</li> <li>dealing with pixels;</li> <li>additional functionalities.</li> </ul>"},{"location":"create_new_skeleton/#defining-a-skeleton","title":"Defining a skeleton","text":"<p>A skeleton can be defined as:</p> <ul> <li>unordered list of keypoints: the list defines the keypoints as pairs of points associated to the keypoint tag;</li> <li> <p>ordered list of keypoints: the list defines the keypoints according to the following order:</p> <ul> <li>0: <code>shoulder_center</code></li> <li>1: <code>head</code></li> <li>2: <code>shoulder_left</code></li> <li>3: <code>elbow_left</code></li> <li>4: <code>hand_left</code></li> <li>5: <code>shoulder_right</code></li> <li>6: <code>elbow_right</code></li> <li>7: <code>hand_right</code></li> <li>8: <code>hip_center</code></li> <li>9: <code>hip_left</code></li> <li>10: <code>knee_left</code></li> <li>11: <code>ankle_left</code></li> <li>12: <code>foot_left</code></li> <li>13: <code>hip_right</code></li> <li>14: <code>knee_right</code></li> <li>15: <code>ankle_right</code></li> <li>16: <code>foot_right</code></li> </ul> </li> <li> <p>property-like structure: the available properties are the following:</p> <ul> <li>type: string containing skeleton's type (\"assistive_rehab::SkeletonStd\").</li> <li>tag: string containing skeleton's tag.</li> <li>transformation: 4 x 4 skeleton's roto-translation matrix.</li> <li>coronal: vector containing skeleton's coronal plane.</li> <li>sagittal: vector containing skeleton's sagittal plane.</li> <li>transverse: vector containing skeleton's transverse plane.</li> <li>skeleton: list containing keypoints with the following subproperties:<ul> <li>tag: string containing keypoint's tag.</li> <li>status: string containing keypoint's status (updated or stale).</li> <li>position: vector containing keypoint's camera coordinates x,y,z.</li> <li>pixel: vector containing keypoint's image coordinates u,v.</li> <li>child: list containing keypoint's child, specified as position, status, tag.</li> </ul> </li> </ul> <p>An example is the following:</p> <pre><code>(coronal (-0.0171187190241832 0.168776145353523 -0.984581522687332)) (sagittal (0.999852451902468 0.0040979367701588 -0.0166817666585492)) (skeleton (((child (((pixel (143.0 18.0)) (position (-0.0958624515848492 -0.655793450456223 2.01140785217285)) (status updated) (tag head)) ((child (((child (((pixel (182.0 106.0)) (position (0.132475296984726 -0.0970469850595488 2.24838447570801)) (status updated) (tag handLeft)))) (pixel (179.0 77.0)) (position (0.126047049721491 -0.301864959631551 2.20449256896973)) (status updated) (tag elbowLeft)))) (pixel (173.0 40.0)) (position (0.0829495294433831 -0.549497736338085 2.17046546936035)) (status updated) (tag shoulderLeft)) ((child (((child (((pixel (117.0 102.0)) (position (-0.318757589652556 -0.129758383247854 2.30834770202637)) (status updated) (tag handRight)))) (pixel (124.0 75.0)) (position (-0.265819393982889 -0.319631634579399 2.22591972351074)) (status updated) (tag elbowRight)))) (pixel (132.0 42.0)) (position (-0.194277581569445 -0.550633963158095 2.17509078979492)) (status updated) (tag shoulderRight)) ((child (((child (((child (((child (((pixel (153.0 200.0)) (position (-0.0305329581785242 0.615750619982796 2.40194511413574)) (status updated) (tag footLeft)))) (pixel (156.0 187.0)) (position (-0.0161527965668162 0.538963623868779 2.52594947814941)) (status updated) (tag ankleLeft)))) (pixel (158.0 148.0)) (position (-0.00361483866030924 0.218990793550177 2.42797660827637)) (status updated) (tag kneeLeft)))) (pixel (164.0 105.0)) (position (0.0249104292284522 -0.102991968370345 2.2304573059082)) (status updated) (tag hipLeft)) ((child (((child (((child (((pixel (138.0 194.0)) (position (-0.170998545149134 0.592533010093213 2.49751472473145)) (status updated) (tag footRight)))) (pixel (144.0 182.0)) (position (-0.130142124436922 0.509498389750101 2.55434036254883)) (status updated) (tag ankleRight)))) (pixel (142.0 147.0)) (position (-0.133305882322194 0.209480672220591 2.39202308654785)) (status updated) (tag kneeRight)))) (pixel (138.0 103.0)) (position (-0.158599348153779 -0.109193487775799 2.21346664428711)) (status updated) (tag hipRight)))) (pixel (151.0 103.0)) (position (-0.0738603465810386 -0.102184160595443 2.20890045166016)) (status updated) (tag hipCenter)))) (pixel (153.0 41.0)) (position (-0.054339762871519 -0.540136057902244 2.13348770141602)) (status updated) (tag shoulderCenter)))) (tag \"#7\") (transformation (4 4 (1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0))) (transverse (0.0438836580749896 -0.984546958261919 -0.169533216601232)) (type \"assistive_rehab::SkeletonStd\")\n</code></pre> </li> </ul>"},{"location":"create_new_skeleton/#from-an-unordered-list-of-keypoints","title":"From an unordered list of keypoints","text":"<p>The following code snippet creates a <code>SkeletonStd</code> object from an unordered list of keypoints:</p> <pre><code>#include &lt;yarp/sig/Vector.h&gt;\n#include \"AssistiveRehab/skeleton.h\"\n\nint main()\n{\n    assistive_rehab::SkeletonStd skeleton;\n    std::vector&lt;std::pair&lt;std::string,yarp::sig::Vector&gt;&gt; unordered;\n    {\n        yarp::sig::Vector p(3); p[0]=0.0; p[1]=0.0; p[2]=0.0;\n        unordered.push_back(std::make_pair(KeyPointTag::shoulder_center,p));\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=0.0; p[1]=-0.1; p[2]=0.0;\n        unordered.push_back(std::make_pair(KeyPointTag::head,p));\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=0.1; p[1]=0.0; p[2]=0.0;\n        unordered.push_back(std::make_pair(KeyPointTag::shoulder_left,p));\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=0.2; p[1]=0.0; p[2]=0.0;\n        unordered.push_back(std::make_pair(KeyPointTag::elbow_left,p));\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=0.3; p[1]=0.0; p[2]=0.0;\n        unordered.push_back(std::make_pair(KeyPointTag::hand_left,p));\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=-0.1; p[1]=0.0; p[2]=0.0;\n        unordered.push_back(std::make_pair(KeyPointTag::shoulder_right,p));\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=-0.2; p[1]=0.0; p[2]=0.0;\n        unordered.push_back(std::make_pair(KeyPointTag::elbow_right,p));\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=-0.3; p[1]=0.0; p[2]=0.0;\n        unordered.push_back(std::make_pair(KeyPointTag::hand_right,p));\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=0.0; p[1]=0.1; p[2]=0.0;\n        unordered.push_back(std::make_pair(KeyPointTag::hip_center,p));\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=0.1; p[1]=0.1; p[2]=0.0;\n        unordered.push_back(std::make_pair(KeyPointTag::hip_left,p));\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=0.1; p[1]=0.2; p[2]=0.0;\n        unordered.push_back(std::make_pair(KeyPointTag::knee_left,p));\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=-0.1; p[1]=0.1; p[2]=0.0;\n        unordered.push_back(std::make_pair(KeyPointTag::hip_right,p));\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=-0.1; p[1]=0.2; p[2]=0.0;\n        unordered.push_back(std::make_pair(KeyPointTag::knee_right,p));\n    }\n\n    skeleton.setTag(\"unordered\");\n    skeleton.update(unordered);\n    skeleton.print();\n\n    return EXIT_SUCCESS;\n}\n</code></pre> <p>The result is:</p> <pre><code>tag = \"unordered\"\ntransformation =\n-1.000  0.000  0.000  0.000\n 0.000  1.000  0.000  0.000\n-0.000  0.000 -1.000  0.000\n 0.000  0.000  0.000  1.000\ncoronal = (-0.000  0.000  1.000)\nsagittal = (-1.000  0.000 -0.000)\ntransverse = ( 0.000 -1.000  0.000)\nkeypoint[\"shoulderCenter\"] = ( 0.000  0.000  0.000); pixel=( nan  nan); status=updated; parent={}; child={\"head\" \"shoulderLeft\" \"shoulderRight\" \"hipCenter\" }\nkeypoint[\"head\"] = ( 0.000 -0.100  0.000); pixel=( nan  nan); status=updated; parent={\"shoulderCenter\" }; child={}\nkeypoint[\"shoulderLeft\"] = (-0.100  0.000 -0.000); pixel=( nan  nan); status=updated; parent={\"shoulderCenter\" }; child={\"elbowLeft\" }\nkeypoint[\"elbowLeft\"] = (-0.200  0.000 -0.000); pixel=( nan  nan); status=updated; parent={\"shoulderLeft\" }; child={\"handLeft\" }\nkeypoint[\"handLeft\"] = (-0.300  0.000 -0.000); pixel=( nan  nan); status=updated; parent={\"elbowLeft\" }; child={}\nkeypoint[\"shoulderRight\"] = ( 0.100  0.000  0.000); pixel=( nan  nan); status=updated; parent={\"shoulderCenter\" }; child={\"elbowRight\" }\nkeypoint[\"elbowRight\"] = ( 0.200  0.000  0.000); pixel=( nan  nan); status=updated; parent={\"shoulderRight\" }; child={\"handRight\" }\nkeypoint[\"handRight\"] = ( 0.300  0.000  0.000); pixel=( nan  nan); status=updated; parent={\"elbowRight\" }; child={}\nkeypoint[\"hipCenter\"] = ( 0.000  0.100  0.000); pixel=( nan  nan); status=updated; parent={\"shoulderCenter\" }; child={\"hipLeft\" \"hipRight\" }\nkeypoint[\"hipLeft\"] = (-0.100  0.100 -0.000); pixel=( nan  nan); status=updated; parent={\"hipCenter\" }; child={\"kneeLeft\" }\nkeypoint[\"kneeLeft\"] = (-0.100  0.200 -0.000); pixel=( nan  nan); status=updated; parent={\"hipLeft\" }; child={\"ankleLeft\" }\nkeypoint[\"ankleLeft\"] = ( nan  nan  nan); pixel=( nan  nan); status=stale; parent={\"kneeLeft\" }; child={\"footLeft\" }\nkeypoint[\"footLeft\"] = ( nan  nan  nan); pixel=( nan  nan); status=stale; parent={\"ankleLeft\" }; child={}\nkeypoint[\"hipRight\"] = ( 0.100  0.100  0.000); pixel=( nan  nan); status=updated; parent={\"hipCenter\" }; child={\"kneeRight\" }\nkeypoint[\"kneeRight\"] = ( 0.100  0.200  0.000); pixel=( nan  nan); status=updated; parent={\"hipRight\" }; child={\"ankleRight\" }\nkeypoint[\"ankleRight\"] = ( nan  nan  nan); pixel=( nan  nan); status=stale; parent={\"kneeRight\" }; child={\"footRight\" }\nkeypoint[\"footRight\"] = ( nan  nan  nan); pixel=( nan  nan); status=stale; parent={\"ankleRight\" }; child={}\n</code></pre> <p>Note</p> <p>Don't get worried about <code>nan</code> in the <code>pixel</code> fields: it's correct since we didn't set them up yet. Uninitialized points and/or pixels take <code>nan</code> values.</p>"},{"location":"create_new_skeleton/#from-an-ordered-list-of-keypoints","title":"From an ordered list of keypoints","text":"<p>The following code snippet creates a <code>SkeletonStd</code> object from an ordered list of keypoints:</p> <pre><code>#include &lt;yarp/sig/Vector.h&gt;\n#include \"AssistiveRehab/skeleton.h\"\n\nint main()\n{\n    assistive_rehab::SkeletonStd skeleton;\n    std::vector&lt;yarp::sig::Vector&gt; ordered;\n    {\n        yarp::sig::Vector p(3); p[0]=0.0; p[1]=0.0; p[2]=0.0;\n        ordered.push_back(p);\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=0.0; p[1]=-0.1; p[2]=0.0;\n        ordered.push_back(p);\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=0.1; p[1]=0.0; p[2]=0.0;\n        ordered.push_back(p);\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=0.2; p[1]=0.0; p[2]=0.0;\n        ordered.push_back(p);\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=0.3; p[1]=0.0; p[2]=0.0;\n        ordered.push_back(p);\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=-0.1; p[1]=0.0; p[2]=0.0;\n        ordered.push_back(p);\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=-0.2; p[1]=0.0; p[2]=0.0;\n        ordered.push_back(p);\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=-0.3; p[1]=0.0; p[2]=0.0;\n        ordered.push_back(p);\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=0.1; p[1]=0.1; p[2]=0.0;\n        ordered.push_back(p);\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=0.1; p[1]=0.2; p[2]=0.0;\n        ordered.push_back(p);\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=-0.1; p[1]=0.1; p[2]=0.0;\n        ordered.push_back(p);\n    }\n    {\n        yarp::sig::Vector p(3); p[0]=-0.1; p[1]=0.2; p[2]=0.0;\n        ordered.push_back(p);\n    }\n\n    skeleton.setTag(\"ordered\");\n    skeleton.update(ordered);\n    skeleton.print();\n\n    return EXIT_SUCCESS;\n}\n</code></pre> <p>The result is:</p> <pre><code>tag = \"ordered\"\ntransformation =\n-1.000  0.000  0.000  0.000\n 0.000  1.000  0.000  0.000\n-0.000  0.000 -1.000  0.000\n 0.000  0.000  0.000  1.000\ncoronal = (-0.000  0.000  0.707)\nsagittal = (-1.000  0.000 -0.000)\ntransverse = ( 0.707 -0.707  0.000)\nkeypoint[\"shoulderCenter\"] = ( 0.000  0.000  0.000); pixel=( nan  nan); status=updated; parent={}; child={\"head\" \"shoulderLeft\" \"shoulderRight\" \"hipCenter\" }\nkeypoint[\"head\"] = ( 0.000 -0.100  0.000); pixel=( nan  nan); status=updated; parent={\"shoulderCenter\" }; child={}\nkeypoint[\"shoulderLeft\"] = (-0.100  0.000 -0.000); pixel=( nan  nan); status=updated; parent={\"shoulderCenter\" }; child={\"elbowLeft\" }\nkeypoint[\"elbowLeft\"] = (-0.200  0.000 -0.000); pixel=( nan  nan); status=updated; parent={\"shoulderLeft\" }; child={\"handLeft\" }\nkeypoint[\"handLeft\"] = (-0.300  0.000 -0.000); pixel=( nan  nan); status=updated; parent={\"elbowLeft\" }; child={}\nkeypoint[\"shoulderRight\"] = ( 0.100  0.000  0.000); pixel=( nan  nan); status=updated; parent={\"shoulderCenter\" }; child={\"elbowRight\" }\nkeypoint[\"elbowRight\"] = ( 0.200  0.000  0.000); pixel=( nan  nan); status=updated; parent={\"shoulderRight\" }; child={\"handRight\" }\nkeypoint[\"handRight\"] = ( 0.300  0.000  0.000); pixel=( nan  nan); status=updated; parent={\"elbowRight\" }; child={}\nkeypoint[\"hipCenter\"] = (-0.100  0.100 -0.000); pixel=( nan  nan); status=updated; parent={\"shoulderCenter\" }; child={\"hipLeft\" \"hipRight\" }\nkeypoint[\"hipLeft\"] = (-0.100  0.200 -0.000); pixel=( nan  nan); status=updated; parent={\"hipCenter\" }; child={\"kneeLeft\" }\nkeypoint[\"kneeLeft\"] = ( 0.100  0.100  0.000); pixel=( nan  nan); status=updated; parent={\"hipLeft\" }; child={\"ankleLeft\" }\nkeypoint[\"ankleLeft\"] = ( 0.100  0.200  0.000); pixel=( nan  nan); status=updated; parent={\"kneeLeft\" }; child={\"footLeft\" }\nkeypoint[\"footLeft\"] = ( nan  nan  nan); pixel=( nan  nan); status=stale; parent={\"ankleLeft\" }; child={}\nkeypoint[\"hipRight\"] = ( nan  nan  nan); pixel=( nan  nan); status=stale; parent={\"hipCenter\" }; child={\"kneeRight\" }\nkeypoint[\"kneeRight\"] = ( nan  nan  nan); pixel=( nan  nan); status=stale; parent={\"hipRight\" }; child={\"ankleRight\" }\nkeypoint[\"ankleRight\"] = ( nan  nan  nan); pixel=( nan  nan); status=stale; parent={\"kneeRight\" }; child={\"footRight\" }\nkeypoint[\"footRight\"] = ( nan  nan  nan); pixel=( nan  nan); status=stale; parent={\"ankleRight\" }; child={}\n</code></pre>"},{"location":"create_new_skeleton/#from-a-property-like-structure","title":"From a property-like structure","text":"<p>Assuming that the object <code>skeleton1</code> has been previously defined using one of the instructions above, a new <code>Skeleton</code> object can be defined as following:</p> <pre><code>    yarp::os::Property prop=skeleton1.toProperty();\n    std::unique_ptr&lt;assistive_rehab::Skeleton&gt; skeleton2(assistive_rehab::skeleton_factory(prop));\n    skeleton2-&gt;setTag(\"properties\");\n    skeleton2-&gt;print();\n</code></pre>"},{"location":"create_new_skeleton/#retrieving-a-skeleton","title":"Retrieving a skeleton","text":"<p>A skeleton can be retrieved as a property-like structure from a yarp port:</p> <pre><code>    assistive_rehab::SkeletonStd skeleton;\n    if(yarp::os::Bottle* b=opcPort.read(false))\n    {\n        yarp::os::Property prop;\n        prop.fromString(b-&gt;get(0).asList()-&gt;toString());\n        skeleton.update(assistive_rehab::skeleton_factory(prop)-&gt;toProperty());\n    }\n</code></pre> <p>or from the OPC as following:</p> <pre><code>    assistive_rehab::SkeletonStd skeleton;\n    yarp::os::Bottle cmd,reply;\n    cmd.addVocab(yarp::os::Vocab::encode(\"ask\"));\n    yarp::os::Bottle &amp;content=cmd.addList().addList();\n    content.addString(\"skeleton\");\n    opcPort.write(cmd,reply);\n    if(reply.size()&gt;1)\n    {\n        if(reply.get(0).asVocab()==Vocab::encode(\"ack\"))\n        {\n            if(yarp::os::Bottle *idField=reply.get(1).asList())\n            {\n                if(yarp::os::Bottle *idValues=idField-&gt;get(1).asList())\n                {\n                    int id=idValues-&gt;get(0).asInt();\n                    cmd.clear();\n                    cmd.addVocab(Vocab::encode(\"get\"));\n                    yarp::os::Bottle &amp;content=cmd.addList().addList();\n                    yarp::os::Bottle replyProp;\n                    content.addString(\"id\");\n                    content.addInt(id);\n                    opcPort.write(cmd,replyProp);\n                    if(replyProp.get(0).asVocab() == yarp::os::Vocab::encode(\"ack\"))\n                    {\n                        if(yarp::os::Bottle *propField=replyProp.get(1).asList())\n                        {\n                            yarp::os::Property prop(propField-&gt;toString().c_str());\n                            skeleton.update(assistive_rehab::skeleton_factory(prop)-&gt;toProperty());\n                        }\n                    }\n                }\n            }\n        }\n    }\n</code></pre>"},{"location":"create_new_skeleton/#accessing-single-keypoints-of-a-skeleton","title":"Accessing single keypoints of a skeleton","text":"<p>If you want to access to a keypoint of the skeleton, you can use the <code>operator[]</code> of the class <code>Skeleton</code>, by passing as parameter the keypoint's tag. For example, the following snippet allows you to get the <code>shoulder_center</code> 3D coordinates:</p> <pre><code>   yarp::sig::Vector sc=skeleton[assistive_rehab::KeyPointTag::shoulder_center]-&gt;getPoint();\n</code></pre>"},{"location":"create_new_skeleton/#dealing-with-pixels","title":"Dealing with pixels","text":"<p>Occasionally, it might be worth storing also the pixels alongside the points, which are used by the algorithm to reconstruct the 3D skeleton. This is particularly useful when the skeleton is employed to enable gaze tracking, for example. In this context, the 3D information of the keypoints needs normally to be transformed from the camera frame to the root frame of the robot. Instead, having the pixels would ease this process. </p> <p>The updating methods described above do have their pixel-wise counterparts:</p> <pre><code>    std::vector&lt;std::pair&lt;yarp::sig::Vector,yarp::sig::Vector&gt;&gt; ordered_withpixels;\n    yarp::sig::Vector p(3,0.1); yarp::sig::Vector px(2,10.0);\n    ordered_withpixels.push_back(std::make_pair(p,px));\n    skeleton.update_withpixels(ordered_withpixels);\n\n    std::vector&lt;std::pair&lt;std::string&lt;std::pair&lt;yarp::sig::Vector,yarp::sig::Vector&gt;&gt;&gt; unordered_withpixels;\n    yarp::sig::Vector p(3,0.1); yarp::sig::Vector px(2,10.0);\n    unordered_withpixels.push_back(std::make_pair(assistive_rehab::KeyPointTag::shoulder_center,std::make_pair(p,px)));\n    skeleton.update_withpixels(unordered_withpixels);\n\n    yarp::sig::Vector pixel=skeleton[assistive_rehab::KeyPointTag::shoulder_center].getPixel();\n</code></pre>"},{"location":"create_new_skeleton/#additional-functionalities","title":"Additional functionalities","text":""},{"location":"create_new_skeleton/#normalization","title":"Normalization","text":"<p>Some applications might require a normalized skeleton, to avoid having different results for different human physiques. The normalization provided in the library makes the length of the observed human segments always equal to 1 and can be applied as following:</p> <pre><code>   skeleton.normalize();\n</code></pre>"},{"location":"create_new_skeleton/#keypoints-reference-system","title":"Keypoints' reference system","text":"<p>Keypoints in the skeleton are defined with respect to the camera. To change the reference system, given a transformation matrix T, you can use the method <code>setTransformation</code>, as following:</p> <pre><code>   skeleton.setTransformation(T);\n</code></pre> <p>Tip</p> <p>If you want to use the skeleton as reference system, you can create the transformation matrix T from the skeleton's planes: <pre><code>   yarp::sig::Vector coronal=skeleton.getCoronal();\n   yarp::sig::Vector sagittal=skeleton.getSagittal();\n   yarp::sig::Vector transverse=skeleton.getTransverse();\n   yarp::sig::Vector p=skeleton[assistive_rehab::KeyPointTag::shoulder_center]-&gt;getPoint();\n   yarp::sig::Matrix T1(4,4);\n   T1.setSubcol(coronal,0,0);\n   T1.setSubcol(sagittal,0,1);\n   T1.setSubcol(transverse,0,2);\n   T1.setSubcol(p,0,3);\n   T1(3,3)=1.0;\n   T=yarp::math::SE3inv(T1);\n   skeleton.setTransformation(T);\n</code></pre></p>"},{"location":"disembodied/","title":"How to run the visual pipeline in a disembodied manner","text":"<p>The visual pipeline can be run also without a physical robot, i.e. with a (disembodied) realsense camera . This tutorial will show you how to do it.</p> <p>Warning</p> <p>The visual pipeline relies on <code>yarpOpenPose</code> and <code>actionRecognizer</code>, which require an NVIDIA graphics card to be run.</p> <p>First, you need to run <code>yarpserver</code>. You can open a terminal and type:</p> <pre><code>yarpserver\n</code></pre> <p>Connect the realsense to your laptop.</p> <p>Seealso</p> <p>If you have not done it, you will need to install it.</p> <p>Warning</p> <p><code>yarp</code> has to be compiled with <code>ENABLE_yarpmod_realsense2 ON</code>.</p> <p>Open <code>yarpmanager</code>, run the Assistive_Rehab_App and hit connect.</p> <p>Note</p> <p>All the modules that require a robot must not be run, i.e. <code>faceDisplayServer</code>, <code>faceExpressionImage</code>, <code>attentionManager</code>, <code>cer_gaze-controller</code>, <code>interactionManager</code>, <code>ctpService</code>, <code>cer_reaching-solver</code> and <code>cer_reaching-controller</code>.</p> <p>Tip</p> <p>You can customize the app by removing unnecessary modules and replacing the nodes in the xml with localhost. You can save the app in the folder $HOME/.local/share/yarp and exploit the shadowing mechanism. In this way, when you open <code>yarpmanager</code>, the app will be automatically loaded.</p> <p>Once all the modules are running, you need to send commands to <code>motionAnalyzer</code>, in order to select the metric and the skeleton that you want to analyze. Therefore you can open a terminal and type:</p> <pre><code>yarp rpc /motionAnalyzer/cmd\n\nloadMetric metric_tag\nselectSkel skeleton_tag\nstart\n</code></pre> <p>where <code>metric_tag</code> and <code>skeleton_tag</code> are respectively the tag of the metric and the tag of the skeleton under analysis (for example ROM_0 and #0).</p> <p>Tip</p> <p>The available metrics can be listed using the command <code>listMetrics</code>.</p> <p>When the command <code>start</code> is given to <code>motionAnalyzer</code>, the visual pipeline starts, i.e. the template skeleton is loaded and visualized on <code>skeletonViewer</code>, the extracted metric is visualized on <code>yarpscope</code> and the action recognition pipeline starts.</p> <p>You should have the following situation:</p> <p></p> <p>A verbal feedback is also provided throughout the experiment.</p> <p>When you want to stop the pipeline, you need to send a stop command to <code>motionAnalyzer</code>, by typing <code>stop</code> in the previous terminal.</p>"},{"location":"gazebo_plugin/","title":"How to use the gazebo plugin","text":"<p>This tutorial will show you how to use the <code>gazebo</code> plugin we developed for an animated model, which is called <code>actor</code> in <code>gazebo</code>.</p> <p>As shown in this gazebo tutorial, an <code>actor</code> in <code>gazebo</code> is a model which can be animated using <code>.dae</code> files and scripted trajectories. Our plugin extends the <code>actor</code> class, allowing the user to play and stop single animations or the whole script, to change the speed on the fly, to reach targets in the environment.</p>"},{"location":"gazebo_plugin/#dependencies","title":"Dependencies","text":"<p>After installing <code>assistive-rehab</code>, you will need the following dependencies:</p> <ul> <li>gazebo: for running the virtual environment;</li> <li>gazebo-yarp-plugins: for exposing YARP interfaces in <code>gazebo</code>;</li> </ul>"},{"location":"gazebo_plugin/#preparing-your-environment","title":"Preparing your environment","text":"<p>The first step you need to take is to prepare your environment.</p> <p>Installing <code>assistive-rehab</code> will result in the shared library <code>libgazebo_assistiverehab_tuginterface.so</code>, which can be included in a <code>gazebo</code> simulation.</p> <p>Following this gazebo tutorial, you need to let <code>gazebo</code> know where the plugin shared library is located. For doing this, you will need to set the following environment variables:</p> <ul> <li><code>GAZEBO_PLUGIN_PATH</code>: has to point to the folder where you installed the shared library;</li> <li><code>GAZEBO_MODEL_PATH</code>: has to point to the folder including the actor <code>.dae</code> files;</li> </ul> <p>gazebo models</p> <p><code>gazebo</code> currently provides several <code>.dae</code> animations, which can be found here.</p> <ul> <li><code>GAZEBO_RESOURCE_PATH</code>: has to point to the folder including your world application.</li> </ul>"},{"location":"gazebo_plugin/#how-to-include-the-plugin-in-your-world","title":"How to include the plugin in your world","text":"<p>To include the plugin in your world, you can add the highlighted lines to the actor tag:</p> <pre><code>&lt;actor name=\"actor\"&gt;\n  &lt;skin&gt;\n    &lt;filename&gt;stand.dae&lt;/filename&gt;\n  &lt;/skin&gt;\n  &lt;animation name=\"stand\"&gt;\n    &lt;filename&gt;stand.dae&lt;/filename&gt;\n  &lt;/animation&gt;\n  &lt;animation name=\"sit_down\"&gt;\n    &lt;filename&gt;sit_down.dae&lt;/filename&gt;\n  &lt;/animation&gt;\n  &lt;animation name=\"stand_up\"&gt;\n    &lt;filename&gt;stand_up.dae&lt;/filename&gt;\n  &lt;/animation&gt;\n  &lt;animation name=\"walk\"&gt;\n    &lt;filename&gt;walk.dae&lt;/filename&gt;\n    &lt;interpolate_x&gt;true&lt;/interpolate_x&gt;\n  &lt;/animation&gt;\n  &lt;script&gt;\n    &lt;loop&gt;false&lt;/loop&gt;\n    &lt;auto_start&gt;false&lt;/auto_start&gt;\n    &lt;trajectory id=\"0\" type=\"stand\"/&gt;\n    &lt;trajectory id=\"1\" type=\"sit_down\"/&gt;\n    &lt;trajectory id=\"2\" type=\"stand_up\"/&gt;\n    &lt;trajectory id=\"3\" type=\"walk\"/&gt;\n  &lt;/script&gt;\n  &lt;plugin name='tug_interface' filename='libgazebo_assistiverehab_tuginterface.so'&gt;\n    &lt;yarpConfigurationFile&gt;model://tugInterface.ini&lt;/yarpConfigurationFile&gt;\n  &lt;/plugin&gt;\n&lt;/actor&gt;\n</code></pre> <p>This example snippet generates an actor associated to three animations, <code>stand.dae</code>, <code>sit_down.dae</code>, <code>stand_up.dae</code> and <code>walk.dae</code>. The plugin loads the <code>tugInterface.ini</code> configuration file and opens a rpc port named by default <code>/tug_input_port</code>, which gives you access to a set of thrift services, described in the next section.</p> <p>Important</p> <p>To use the plugin you will need to have an actor with at least one animation associated.</p> <p>Note</p> <p>We are going to refer to this snippet in next section to see the plugin functionalities, but you can personalize the actor according to your needs.</p>"},{"location":"gazebo_plugin/#using-the-plugin","title":"Using the plugin","text":"<p>To start using the plugin, first open a terminal and run <code>yarpserver</code>. Open a new terminal and type <code>gazebo</code> followed by the name of your scenario.</p> <p>Remember</p> <p><code>gazebo</code> looks for world files in the <code>$GAZEBO_RESOURCE_PATH</code> environment variable, as described at the beginning of this tutorial.</p> <p>Now you should see an actor standing on your screen:</p> <p></p> <p>You can start playing with the plugin!</p> <p>Open a terminal and type <code>yarp rpc /tug_input_port</code>.</p>"},{"location":"gazebo_plugin/#playing-the-animations","title":"Playing the animations","text":"<p>Type <code>play</code> on the terminal. All the animations are going to be played in a row, following the sequence defined by the ids in the trajectories as defined here:</p> <p></p> <p>Additionally, you can play the single animations associated to the actor or play the whole script from the specified animation:</p> <p> PLAY SINGLE ANIMATION </p> <p> PLAY FROM ANIMATION </p> <code>play stand</code> <p> </p> <code>play stand -1 true</code> <p> </p> <code>play sit_down</code> <p> </p> <code>play sit_down -1 true</code> <p> </p> <code>play stand_up</code> <p> </p> <code>play stand_up -1 true</code> <p> </p> <code>play walk</code> <p> </p> <code>play walk  -1 true</code> <p> </p> <p>Retrieving animations</p> <p>You can retrieve the list of animations associated to your actor using the rpc command <code>getAnimationList</code>.</p> <p>Example</p> <p>Playing a single animation of the script is useful when you need a condition to be accomplished before the specific animation is played. For example, in the TUG scenario, we want the actor to sit or to start walking only when the robot gives the related command. You can check it out here.</p>"},{"location":"gazebo_plugin/#changing-walking-parameters","title":"Changing walking parameters","text":"<p>The path followed during the <code>walk</code> animation is specified by <code>targets</code> in the <code>tugInterface.ini</code> configuration file: this is a matrix containing the poses to reach in the form <code>x y theta</code>.</p> <p>Reference frame</p> <p>The targets are defined with respect to the gazebo world frame, thus <code>X</code> pointing forward (with respect to the human model), <code>Y</code> pointing left, <code>Z</code> pointing up.</p> <p>For our application, we need the human model to reach the target and go back to the initial position. You can control the trajectory through the following rpc commands:</p> <p> DESCRIPTION </p> <p> OUTPUT </p> <p> <code>setTarget x y theta</code>:  providing <code>x y theta</code>, when playing the <code>walk</code> animation, the human model will reach the new specified target and go back. </p> - <code>setTarget 3.5 2.0 10.0</code>  - <code>play walk</code> <p> </p> <p> <code>goToSeq (x1 y1 theta1 ... xN yN thetaN)</code>:  providing the list of <code>xi yi thetai</code>, the human model reaches the specified waypoints. </p> - <code>goToSeq (3.0 2.0 20.0 5.0 0.0 -20.0)</code> <p> </p> <p>Note</p> <p>The <code>goToSeq</code> is a blocking service: only when the last target is reached, an ack is returned.</p> <p>Tip</p> <p>The interface provides two additional services, <code>goTo</code> and <code>goToWait</code> which are the punctual versions of <code>goToSeq</code>, respectively non blocking and blocking.</p> <p>Finally, you can set the speed of the walking animation through the command <code>setSpeed</code>:</p> <p> WALKING SLOWER </p> <p> WALKING FASTER </p> - <code>setSpeed 0.5</code>  - <code>play walk</code> <p> </p> - <code>setSpeed 2.0</code>  - <code>play walk</code> <p> </p>"},{"location":"gazebo_plugin/#additional-services","title":"Additional services","text":"<p>The additional services provided are the following:</p> <ul> <li><code>playFromLast</code>: to play the animation from the last stop. A stop can be provided while an animation is being played. With this command, you will be able to start the script exactly from where you stopped.</li> </ul> <p>Note</p> <p>The whole script is played by default. The command <code>playFromLast false</code> plays only the animation coming after last stop.</p> <ul> <li><code>getState</code>: to know the current animation being played;</li> <li><code>pause</code>: to pause the actor for a specified time or for an unlimited time (if not specified and until the <code>play</code> command is provided).</li> </ul>"},{"location":"install/","title":"Install","text":"<p>Disclaimer</p> <p>Assistive-rehab has been widely tested on <code>Ubuntu 16.04</code> and <code>Ubuntu 18.04</code>. If you face any issue either with your OS, please submit an Issue.</p>"},{"location":"install/#requirements","title":"Requirements","text":"<ul> <li>Supported Operating Systems: Linux, Windows, macOS</li> <li>C++11 compiler</li> <li>CMake 3.5</li> <li>ycm</li> <li>icub-contrib-common</li> <li>yarp (3.1.100 or higher)</li> <li>iCub</li> <li>OpenCV (3.4.0 or higher)</li> <li>yarpOpenPose</li> <li>Ipopt</li> <li>yarp.js</li> </ul> <p>yarp</p> <ul> <li><code>ENABLE_yarpcar_mjpeg ON</code>: to allow mjpeg compression.</li> <li><code>ENABLE_yarpcar_zfp ON</code>: to allow zfp compression.</li> <li><code>ENABLE_yarpmod_realsense2 ON</code>: to enable the realsense.</li> </ul> <p>OpenCV</p> <ol> <li>Download OpenCV: <code>git clone https://github.com/opencv/opencv.git</code>.</li> <li>Checkout the correct branch/tag: <code>git checkout 3.4.0</code>.</li> <li>Download the external modules: <code>git clone https://github.com/opencv/opencv_contrib.git</code>.</li> <li>Checkout the same branch/tag: <code>git checkout 3.4.0</code>.</li> <li>Configure OpenCV by filling in <code>OPENCV_EXTRA_MODULES_PATH</code> with the path to opencv_contrib/modules and then toggling on all possible modules.</li> <li>Compile OpenCV.</li> </ol>"},{"location":"install/#optional-dependencies","title":"Optional dependencies","text":"Dependency License TensorFlowCC MIT fftw3 GPL GSL GPL matio BSD 2-Clause VTK (8.1.0 or higher) BSD-style cer GPL <p>TensorFlowCC</p> <p>TensorFlowCC builds and installs the TensorFlow C++ API, which is released under Apache 2.0 license.</p> <p>matio</p> <p>On <code>Ubuntu 18.04</code>, you can install the library through apt: <code>sudo apt install libmatio-dev</code>.</p> <p>Warning</p> <p>If an optional dependency is not found, the modules depending on it are not compiled.</p> <p>Failure</p> <p>If you want to run the full demo, also additional dependencies are required.</p> <p>report</p> <p>For generating the offline report, you will need to install the following python libraries (you can install them through <code>pip install</code>):</p> <ul> <li>scipy</li> <li>numpy</li> <li>matplotlib</li> <li>pandas</li> <li>glob</li> <li>jupyter</li> <li>plotly</li> <li>warnings</li> <li>collections</li> <li>math</li> <li>re</li> <li>os</li> <li>datetime</li> <li> <p>IPython</p> <p>plotly</p> <p>You need to enable jupyter extension to allow plotly to work in jupyter notebook: <code>pip install \"notebook&gt;=5.3\" \"ipywidgets&gt;=7.2\" --user</code>.    </p> </li> </ul>"},{"location":"install/#installation","title":"Installation","text":"<p>If all the dependencies are met, proceed with the following instructions:</p> <p>From sources</p> <p>Substitute to <code>&lt;install-prefix&gt;</code> the absolute path where you want to install the project.</p> <p>````tab=\"GNU/Linux and macOS\" git clone robotology/assistive-rehab.git mkdir build &amp;&amp; cd build cmake .. -DCMAKE_INSTALL_PREFIX= make make install <pre><code>````tab=\"Windows\"\ngit clone https://github.com/robotology/assistive-rehab.git\nmkdir build &amp;&amp; cd build\ncmake .. -DCMAKE_INSTALL_PREFIX=&lt;install-prefix&gt;\nmake\nmake install\n</code></pre>"},{"location":"latest_news/","title":"Latest news","text":"<p>May 4, 2021 : Checkout our latest release v0.6.0!</p> <p>What's new?</p> <ul> <li>We developed the <code>obstacleDetector</code> module which clusters data received from front and back lasers, by means of Euclidean distance and stops the navigation when the robot reaches a threshold distance (<code>1.5 m</code>) from the closest obstacle: </li> </ul> <p></p> <ul> <li>The obstacle detection has been included into the clinical test Timed Up and Go (TUG)! If an obstacle is found within a radius of <code>1.5 m</code> around the robot (within the rear and front laser FOVs), the interaction is frozen and the robot asks to remove the obstacle. The interaction does not start until the obstacle is removed (within a timeout).</li> </ul> <p>Checkout the video:</p> <p></p> <p>May 22, 2020 : Checkout our latest release v0.5.0! </p> <p>What's new?</p> <ul> <li>The clinical test Timed Up and Go (TUG) is now ready for both the real robot and within the simulation environment <code>gazebo</code>:</li> </ul> <p></p> <p>Follow the tutorial to run the demo in <code>gazebo</code>!</p> <p>Tip</p> <p>Click on the image to open the video!</p> <ul> <li> <p>The motion analysis has been extended to the lower limbs: now we can evaluate walking parameters, such as step length and width, walking speed and number of steps.</p> </li> <li> <p>We developed the <code>lineDetector</code> module to visually detect start and finish lines on the floor, composed of ArUco markers.</p> </li> <li> <p>We developed a reactive navigation system, which allows the robot to navigate based on the received perceptual stimuli: the robot can reach fixed points in the environment (such as the start and the finish lines) and follow the user maintaining a fixed distance along a straight path.</p> </li> </ul> <p>Note</p> <p>The environment is supposed to be free from obstacles.</p> <ul> <li> <p>We integrated the Google services API within our application to have a simple natural question and answer mechanism:</p> <ol> <li>the <code>googleSpeech</code> module receives the sound from a microphone and retrieves the speech transcript from Google Speech cloud services;</li> <li>the <code>googleSpeechProcess</code> module receives the speech transcript and analyses the sentence to retrieve its structure and meaning, relying on Google Language Cloud services.</li> </ol> </li> <li> <p>The speech system can be triggered by a Mystrom wifi button, which avoids the system to be always listening and thus responsive also to background noise: whenever the user presses the button, the robot is ready to answer questions in italian!</p> </li> </ul> <p>Note</p> <p>The robot can answer a selected set of questions related to the TUG. The interaction is still flexible as questions can be posed by the user in natural language, thanks to the capability of the system to interpret the question, rather than simply recognize it.</p> <p>March 19, 2020 : Added new tutorial to detect Aruco boards in <code>gazebo</code>. Check it out!</p> <p>July 10, 2019 : Checkout our latest release v0.4.0!</p> <p>What's new?</p> <ul> <li>the feedback can be provided now using the robot skeleton template, rather than the pre-recorded one. The new module <code>robotSkeletonPublisher</code> publishes the robot skeleton, which represents R1 limbs configuration, as following:</li> </ul> <p></p> <p>The robot skeleton is remapped onto the observed skeleton internally within <code>feedbackProducer</code> for the further analysis (<code>skeletonScaler</code> and <code>skeletonPlayer</code> are thus bypassed). Such modality insures a full synchronization between the robot movement and the skeleton template, which was not guaranteed with the pre-recorded template.</p> <p>Note</p> <p>The modality with the pre-recorded template is still available and can be set through <code>interactionManager</code> by setting the flag <code>use-robot-template</code> to <code>false</code>. In such case, the pipeline including <code>skeletonScaler</code> and <code>skeletonPlayer</code> is used.</p> <p>Tip</p> <p>The robot skeleton can be replayed offline by saving the robot joints specified in this app. A tutorial for replaying a full experiment can be found in the Tutorial section.</p> <ul> <li> <p>the Train With Me study aims at comparing users' engagement during a physical training session with a real robot and a virtual agent. Preliminary experiments were designed for comparing R1 with its virtual counterpart and the developed infrastructure is now available. <code>interactionManager</code> can deal with the following three phases:</p> <ol> <li>observation: the real/virtual robot shows the exercise and the user observes it;</li> <li>imitation: the real/virtual robot performs the exercise and the user imitates it,</li> <li>occlusion: the real/virtual robot keeps performing the exercise behind a panel and the user keeps imitating it, without having any feedback.</li> </ol> <p>The scripts used during experiments can be found here, namely <code>AssistiveRehab-TWM-robot.xml.template</code> and <code>AssistiveRehab-TWM-virtual.xml.template</code>, which load parameters defined in the <code>train-with-me</code> context. A tutorial for running the demo with the virtual R1 can be found in the Tutorial section.</p> </li> </ul> <p>May 6, 2019 : Checkout our latest release v0.3.0!</p> <p>This is a major change which refactors the entire framework to deal also with feet, following up the use of <code>BODY_25</code> model of <code>OpenPose</code>. The following is an example of skeleton with feet in 2D and 3D:</p> <p></p> <p>Changes include:</p> <ul> <li><code>SkeletonStd</code> now includes <code>hip_center</code>, <code>foot_left</code>, <code>foot_right</code>:<ul> <li><code>hip_center</code> is directly observed if available, otherwise is estimated as middle point between <code>hip_left</code> and <code>hip_right</code> (the same stands for <code>shoulder_center</code>);</li> <li><code>foot_left</code> and <code>foot_right</code> are detected as being the big-toe. If big-toe is not available, small-toe is used as fallback;</li> </ul> </li> <li><code>SkeletonWaist</code> has been removed in favor of the new <code>SkeletonStd</code>;</li> <li>optimization performed by <code>skeletonRetriever</code> is now extended also to lower limbs;</li> <li>modules previously relying on <code>SkeletonWaist</code> have been updated to use the new <code>SkeletonStd</code>;</li> <li>the new framework is compatible with the Y1M5 demo, which was successfully tested online on the robot;</li> <li>the new framework is compatible with datasets recorded before the release, which can be reproduced by means of <code>yarpdataplayer</code>.</li> </ul> <p>May 6, 2019 : Checkout our new release v0.2.1!</p> <p>What's new?</p> <ul> <li>the action recognition is now robust to rotations! The original network was trained with skeletons frontal to the camera, which is not guaranteed during a real interaction. The network has been re-trained with a wider training set, comprising synthetic rotations applied to real data around each axis, with variation of <code>10</code> degrees in a range of <code>[-20,20]</code> degrees. Also a variability on the speed was introduced in the training set, by considering the action performed at normal, double and half speed. We compared the accuracy of the previous and the new model for different rotations of the skeleton, and results show a high accuracy to a wider range for all axes:  </li> </ul> <p> ROTATION AROUND X </p> Original New <p> </p> <p> </p> <p> ROTATION AROUND Y </p> Original New <p> </p> <p> </p> <p> ROTATION AROUND Z </p> Original New <p> </p> <p> </p> <ul> <li>the skeleton now also stores the pixels alongside the 3D points! This is very useful when using the skeleton for gaze tracking, as it avoids the transformation from the camera frame to the root frame of the robot required is using 3D information;</li> <li>the offline report is now interactive! The user can navigate through plots, zoom, pan, save: </li> </ul> <p>January 28, 2019 : Checkout our latest tutorial on how to run the main applications on the robot R1!</p> <p>January 25, 2019 : We are public now!</p> <p>December 21, 2018 : Checkout the latest release and the the comparison with the first release!</p>"},{"location":"license/","title":"License","text":""},{"location":"license/#bsd-3-clause","title":"BSD 3-Clause","text":"<p>BSD 3-Clause License</p> <p>Copyright \u00a9 2018, Robotology All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ul> <li> <p>Redistributions of source code must retain the above copyright notice, this   list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright notice,   this list of conditions and the following disclaimer in the documentation   and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its   contributors may be used to endorse or promote products derived from   this software without specific prior written permission.</p> </li> </ul> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"main_apps/","title":"How to run the rehabilitation demos","text":"<p>This tutorial will show you how to run the rehabilitation demos.</p> <p>The applications can be found here, namely AssistiveRehab.xml.template and AssistiveRehab-faces.xml.template.</p>"},{"location":"main_apps/#the-basic-app-assistiverehabxml","title":"The basic app: AssistiveRehab.xml","text":"<p>Let's start with the basic one: AssistiveRehab.xml.template.</p> <p>Tip</p> <p>AssistiveRehab-faces.xml.template builds upon AssistiveRehab.xml.template, introducing additional modules for face recognition.</p> <p>We assume you are working on the robot R1 and that <code>r1-face</code>, <code>r1-torso1</code>, <code>r1-cuda-linux</code>, <code>r1-console-linux</code>, <code>r1-base</code> and <code>r1-display-linux</code> are available.</p> <p>R1</p> <p>On R1, <code>yarpserver</code> runs on <code>r1-base</code>.</p> <p>R1-mk2</p> <p>On R1-mk2, the cuda system is named <code>r1-console-cuda</code>, which we also use as display (therefore <code>r1-display-linux</code> is <code>r1-console-cuda</code>).</p> <p>The interaction requires the robot's motors to be on. Therefore turn on the motors, then open a terminal and type:</p> <pre><code>ssh r1-base\ncd $ROBOT_CODE/cer/app/robots/CER02\nyarprobotinterface\n</code></pre> <p>Now we are ready to run the application! Open a new terminal and type:</p> <pre><code>yarpmanager\n</code></pre> <p>Now click on Assistive_Rehab_App, hit run and then connect. The demo is now running! To make the robot look around and engage the user, just issue the <code>start</code> command to the <code>interactionManager</code> module. If the user accepts the invitation by lifting her/his hand, the interaction starts. The user has to repeat the exercise shown by the robot, which in turns evaluates how the exercise is being performed, through a verbal feedback. The session ends with the robot giving an overall feedback of the exercise. The interaction can go on, as the robot keeps memory of the past interaction.</p> <p>Let's look a bit deeper into the application to see which are the modules running:</p> <ul> <li><code>yarpdev --device speech --lingware-context speech --default-language it-IT --robot r1 --pitch 80 --speed 110</code>: to run the speech, by default in italian. You have to change the default-language to en-GB to switch to english;</li> <li><code>yarpdev --device faceDisplayServer</code>: to run the face;</li> <li><code>yarpdev --device --context AssistiveRehab --from realsense2.ini</code>: to run the camera;</li> <li><code>faceExpressionImage</code>: to visualize the expressions and the talking mouth on the robot's display;</li> <li><code>iSpeak --package speech-dev</code>: to make the robot speak;</li> <li><code>yarpOpenPose</code>: to detect 2D skeletons;</li> <li><code>objectsPropertiesCollector</code>: to store 3D skeletons within a yarp-oriented database;</li> <li><code>skeletonRetriever</code>: to produce 3D skeletons from depth and 2D skeletons;</li> <li><code>attentionManager</code>: to make the robot focus and follow a skeleton;</li> <li><code>cer_gaze-controller</code>: to control the robot's gaze;</li> <li><code>motionAnalyzer --from motion-repertoire-rom12.ini</code>: to analyze motion. This configuration file does not include the reaching exercise, which is under testing from the robot's point of view (the motion analysis is implemented);</li> <li><code>skeletonPlayer</code>: to replay the template skeleton;</li> <li><code>skeletonScaler</code>: to move/rescale the template skeleton;</li> <li><code>feedbackProducer</code>: to produce a feedback depending on the exercise;</li> <li><code>actionRecognizer</code>: to recognize the exercise being performed;</li> <li><code>feedbackSynthetizer</code>: to generate a verbal feedback;</li> <li><code>yarpview</code>: to visualize depth and RGB image with 2D skeletons;</li> <li><code>skeletonViewer</code>: to visualize 3D skeletons;</li> <li><code>yarpscope</code>: to visualize the metric on the movement in real-time;</li> <li><code>interactionManager</code>: to supervise the interaction;</li> <li><code>ctpService</code> (for each arm): to send commands to the robot's arms;</li> <li><code>cer_reaching-solver</code> + <code>cer_reaching-controller</code> (for each arm): to let the robot performing a reaching task.</li> </ul> <p>When you want to stop the interaction, hit stop in <code>yarpmanager</code>. You can produce two final reports (in italian and english) of the performed exercises by opening a terminal and typing:</p> <pre><code>assistive-rehab-generate-report.sh\n</code></pre> <p>report</p> <p>For generating the offline report, you will need to install the following python libraries (you can install them through <code>pip install</code>):</p> <ul> <li>scipy</li> <li>numpy</li> <li>matplotlib</li> <li>pandas</li> <li>glob</li> <li>jupyter</li> <li> <p>plotly</p> <p>plotly</p> <p>You need to enable jupyter extension to allow plotly to work in jupyter notebook: <code>pip install \"notebook&gt;=5.3\" \"ipywidgets&gt;=7.2\" --user</code>.</p> </li> </ul> <p>Two html files will be created in the folder where you run the script.</p> <p>Note</p> <p>Producing the report might take a while, as all the files whose names equal the name of the most recent file are processed. This allows us to create a report not only of the current session, but also of the clinical evolution of the patient.</p>"},{"location":"main_apps/#the-basic-app-integrated-with-face-recognition-assistiverehab-facesxml","title":"The basic app integrated with face recognition: AssistiveRehab-faces.xml","text":"<p>The AssistiveRehab-faces.xml.template builds upon the basic app, introducing additional modules for face recognition, which are the following:</p> <p>R1-mk2</p> <p>On R1-mk2, the face recognition pipeline is run on <code>r1-torso2</code>, which has to be available.</p> <ul> <li><code>humanStructure</code>: to restrict the area of recognition around human faces;</li> <li><code>caffeCoder</code>: to extract a representation of the cropped image;</li> <li><code>linearClassifierModule</code>: to classify the extracted features;</li> <li><code>faceRecognizer</code>: to label human faces.</li> </ul> <p>For running the demo, click on Assistive_Rehab_with_Faces_App in <code>yarpmanager</code>, hit run and then connect.</p> <p>Note</p> <p><code>caffeCoder</code> takes a while to load the network. Therefore, before hitting connect, wait some seconds. You can click the refresh icon and check when the <code>caffeCoder</code> ports become green.</p> <p>The demo is similar to the basic one, but now the robot interacts with people using their names!</p> <p>Tip</p> <p>If you want to train the network with your face to the face database, you can use the provided API.</p>"},{"location":"replay_an_experiment/","title":"How to save and replay an experiment","text":"<p>This tutorial will show you how to save and replay an experiment.</p>"},{"location":"replay_an_experiment/#saving-an-experiment","title":"Saving an experiment","text":"<p>You can save an experiment by running this application, which runs several <code>yarpdatadumper</code> for saving:</p> <ol> <li>data from the camera and <code>yarpOpenPose</code>: 2D skeleton data, depth image and RGB image with 2D skeletons.</li> <li>robot joints.</li> </ol> <p>Saved data can be found in the folder skeletonDumper.</p> <p>Note</p> <p>You can save the data from the virtual robot by running this application.</p>"},{"location":"replay_an_experiment/#replaying-an-experiment","title":"Replaying an experiment","text":"<p>After saving an experiment as previously explained, this application allows you to replay it, i.e. to visualize the saved depth, the 2D skeletons, retrieve and visualize the 3D skeletons, publish and visualize the robot skeleton.</p> <p>Tip</p> <p>The application is conceived to visualize saved data. You can run any additional module that takes as input the saved data.  </p> <p>It runs the following modules:</p> <ul> <li><code>objectsPropertiesCollector</code>: to store 3D skeletons within a yarp-oriented database;</li> <li><code>skeletonRetriever</code>: to produce 3D skeletons from depth and 2D skeletons;</li> <li><code>robotSkeletonPublisher</code>: to produce 3D robot skeleton from the saved joints;</li> <li><code>skeletonViewer</code>: to visualize 3D skeletons;</li> <li><code>yarpview</code>: to visualize depth and RGB image with 2D skeletons.</li> </ul> <p>Tip</p> <p>If you are only interested in the visual pipeline from the camera, you can avoid running <code>robotSkeletonPublisher</code>.</p> <p>To run the application, you need a <code>yarpserver</code>. Open a terminal and type:</p> <pre><code>yarpserver\n</code></pre> <p>You need a <code>yarpdataplayer</code> to reproduce the dataset you have previously acquired. Open a terminal and type:</p> <pre><code>yarpdataplayer\n</code></pre> <p>An interface appears, you can click on File, then Open Directory. Click on the folder where you have saved your data (the folder should contain subfolders with 2D skeleton data, depth image and RGB image with 2D skeletons) and press play.</p> <p>Finally open <code>yarpmanager</code>, click on the Assistive_Rehab_Replay_App, hit run and then connect.</p> <p>Note</p> <p><code>robotSkeletonPublisher</code> automatically connects to the robot ports. If you saved the data from the virtual robot using this app, you will need to specify the parameter <code>--robot SIM_CER_ROBOT</code>.</p> <p>Tip</p> <p>Replaying an experiment is an important feature, with the twofold objective of:</p> <ul> <li>allowing the physiotherapist to compute new metrics on the replayed experiment not run online;</li> <li>performing code debbuging.</li> </ul>"},{"location":"tug_demo/","title":"How to run the TUG demo","text":"<p>This tutorial will show you how to run the TUG demo on the real robot and within the simulation environment <code>gazebo</code>.</p>"},{"location":"tug_demo/#running-the-tug-demo-on-the-robot","title":"Running the TUG demo on the robot","text":""},{"location":"tug_demo/#hardware-requirements","title":"Hardware requirements","text":"<p>The following hardware is required:</p> <ul> <li>NVIDIA graphics card: for running <code>yarpOpenPose</code>;</li> <li> <p>Mystrom wifi button: for triggering the speech pipeline;</p> <p>Alternatively</p> <p>The speech pipeline can be alternatively triggered by the hand up / down option (see below)</p> </li> <li> <p>Setup for external microphone</p> </li> </ul>"},{"location":"tug_demo/#setting-up-the-robot","title":"Setting up the robot","text":"<p>Turn on the robot, by switching on cpu and motors. Start <code>yarpserver</code> and <code>yarprun</code> on the robot machines:</p> <ul> <li>on r1-base:<ul> <li><code>yarpserver</code></li> <li><code>yarprun --server /r1-base</code></li> </ul> </li> <li>on r1-console:<ul> <li><code>yarprun --server /r1-console</code></li> </ul> </li> <li>on r1-console-2 (iiticublap194):<ul> <li><code>yarprun --server /r1-console-2</code></li> </ul> </li> <li>on r1-face:<ul> <li><code>yarprun --server /r1-face</code></li> </ul> </li> <li>on r1-torso:<ul> <li><code>yarprun --server /r1-torso</code></li> </ul> </li> </ul> <p>Now that yarp is running, open a terminal and type:</p> <pre><code>ssh -X r1-base\ncd /usr/local/src/robot/robots-configuration/R1SN003 &amp;&amp; yarprobotinterface\n</code></pre> <p>R1SN003</p> <p>These instructions are tailored to the robot <code>R1SN003</code>.    Specifically, this robot has the following systems available:</p> <ul> <li><code>r1-base</code>: where <code>yarpserver</code> and the navigation modules run;</li> <li><code>r1-console</code>: where the speech modules and <code>yarpOpenPose</code> run;</li> <li><code>r1-console-2</code>: where the main modules and the visualizers run;</li> <li><code>r1-torso</code>: where the camera runs;</li> <li><code>r1-face</code>: where the speak and the face expression modules run;  </li> </ul>"},{"location":"tug_demo/#running-the-demo","title":"Running the demo","text":"<p>Now that the system is up and running, we are ready to run the demo!</p>"},{"location":"tug_demo/#using-docker","title":"Using docker","text":"<p>We have setup the whole demo through <code>docker</code> to avoid installation and dependencies on the state of the system.</p> <p>Not familiar with docker?</p> <p>Check it out.</p> <p>Run <code>yarpmanager</code> on <code>r1-console</code>. Here you will find the <code>AssistiveRehab-TUG_setup</code> xml to run the robot camera and the <code>iSpeak</code> module. Hit run all and then connect all.</p> <p>Connections</p> <p>You will notice that the application includes all the connections required for the demo, not only those used to setup the camera and the speech modules. Since the demo is setup through <code>docker-compose</code>, this is a convenient visual way to verify that all the connections started properly. </p> <p>Demo dependencies</p> <p>All the demo specific dependencies are installed within docker containers, so you don't need any extra installation.</p> <p>Let's run the navigation modules on r1-base:</p> <pre><code>ssh -X r1-base\ncd $ROBOT_CODE/assistive-rehab/docker/compose \ndocker-compose -f docker-compose-nav.yml up\n</code></pre> <p>Let's run the main modules and the visualizers on r1-console-2:</p> <pre><code>cd $ROBOT_CODE/assistive-rehab/docker/compose\ndocker-compose -f docker-compose-console.yml up  \n</code></pre> <p>Let's run the speech modules and <code>yarpOpenPose</code> on r1-console:</p> <pre><code>cd $ROBOT_CODE/assistive-rehab/docker/compose\ndocker-compose -f docker-compose-cuda.yml up\n</code></pre> <pre><code>cd $ROBOT_CODE/assistive-rehab/docker/compose\ndocker-compose -f docker-compose-speech.yml up\n</code></pre> <p>Check the connections</p> <p>The connections with <code>docker-compose</code> take some time. You can check that they are all green in <code>yarpmanager</code> by refreshing the application.</p> <p>Debugging docker containers</p> <p>Something went wrong? You can fetch the output of a container using the command <code>docker logs -f &lt;name_of_the_container&gt;</code>. The <code>&lt;name_of_the_container&gt;</code> usually appends the name of the folder where you run <code>docker-compose</code>, the name of the service as defined in the <code>docker-compose.yml</code> and a random number.  </p> <p>Invalid address</p> <p>If you notice that some ports are red in <code>yarpmanager</code>, it's probably due to an <code>invalid address</code> error on that port. It might due to how docker handles the network, so please make sure that each <code>docker-compose</code> is up before running the next.</p> <p>Stopping the demo</p> <p>For stopping, you can repeat the same procedure and using <code>docker-compose -f &lt;name-of-the-file&gt; down</code>.</p>"},{"location":"tug_demo/#alternative-to-docker","title":"Alternative to docker","text":"<p>Be sure to meet all the dependencies defined here.</p> <p>In <code>yarpmanager</code>, click on <code>Applications</code> and then <code>Assistive Rehabilitation TUG App</code> app, hit run and then connect.  The demo is now running!</p>"},{"location":"tug_demo/#instructions-to-start-the-demo","title":"Instructions to start the demo","text":"<p>Before starting the TUG, we need to define our world made of lines, robots and skeletons! For doing so, you can follow this tutorial.</p> <p>Let's run a TUG session!</p> <p>Want to ask questions to the robot?</p> <p>Remember to be equipped the WiFi button, the external microphone and the Virtual Machine running.</p> <p>Alternative to WiFi button</p> <p>The speech pipeline can be alternatively triggered by the hand up / down option: the 3D skeleton is analyzed to detect if the hand is raised up / dropped down and consequently a start / stop is sent to the speech pipeline (the hand has to stay up during the question). This can be achieved by running <code>managerTUG</code> with the option <code>--detect-hand-up true</code>. However, such approach is less robust than using the button for two reasons:</p> <ol> <li>the person is assumed to be always in the FOV, which cannot be guaranteed in a HRI scenario;</li> <li>it may lead to false detection and thus trigger the speech when not required.</li> </ol> <p>Open a terminal and type:</p> <p>```tab=\"Terminal 1\" yarp rpc /managerTUG/cmd:rpc start <pre><code>The robot will move to the starting position defined by `starting-pose` [here](https://github.com/robotology/assistive-rehab/blob/master/modules/managerTUG/app/conf/config-it.ini).\nWhen the robot has reached this point, the interaction starts!\n\nThe interaction keeps going on, with the robot keeping engaging the user.\nYou can stop it by typing in `Terminal 1`:\n\n```tab=\"Terminal 1\"\nstop\n</code></pre></p>"},{"location":"tug_demo/#running-the-tug-demo-on-gazebo","title":"Running the TUG demo on <code>gazebo</code>","text":""},{"location":"tug_demo/#hardware-requirements_1","title":"Hardware requirements","text":"<p>The following hardware is required:</p> <ul> <li>NVIDIA graphics card: for running <code>yarpOpenPose</code>;</li> </ul>"},{"location":"tug_demo/#optional","title":"Optional","text":"<p>If you want to simulate the speech interaction, you will need:</p> <ul> <li>access to Google Cloud API: for running the <code>speechInteraction</code> modules;</li> </ul> <p>Warning</p> <p>You will need to have an account to access Google Cloud API.</p> <ul> <li>a Mystrom wifi button.</li> </ul> <p>How to configure the button</p> <p>On the robot, the WiFi button is already configured to send triggers within the robot network. To configure your own button to run within your network, follow this tutorial.</p>"},{"location":"tug_demo/#running-the-demo_1","title":"Running the demo","text":""},{"location":"tug_demo/#using-docker_1","title":"Using docker","text":"<p>You will simply need to run the following commands:</p> <pre><code>cd $ROBOT_CODE/assistive-rehab/docker/compose\ndocker-compose -f docker-compose-gazebo.yml up  \n</code></pre> <p>On a cuda machine, run:</p> <pre><code>cd $ROBOT_CODE/assistive-rehab/docker/compose\ndocker-compose -f docker-compose-cuda.yml up\n</code></pre>"},{"location":"tug_demo/#alternative-to-docker_1","title":"Alternative to docker","text":"<p>After installing <code>assistive-rehab</code>, you will need the following dependencies:</p> <ul> <li>cer: for running the gaze-controller and face expressions;</li> <li>navigation: for controlling the robot wheels;</li> <li>gazebo: for running the virtual environment;</li> </ul> <p>Work in progress</p> <p>This is a fork of <code>gazebo</code> which contains the required changes to the <code>Actor</code> class. We are working to open a pull request on the upstream repository.</p> <ul> <li>gazebo-yarp-plugins: for exposing YARP interfaces in <code>gazebo</code>;</li> <li>cer-sim: which includes the model loaded by <code>gazebo</code> in <code>tug-scenario.world</code>;</li> <li>speech: for running the <code>iSpeak</code> module;</li> <li>nodejs: for handling triggers from the wifi button.</li> </ul> <p>Now that all dependencies and requirements are met, you are ready to run the demo in <code>gazebo</code>!</p> <p>The first step is to open a terminal and run <code>yarpserver</code>. Open <code>yarpmanager</code>, run the <code>AssistiveRehab-TUG_SIM App</code> and hit connect.</p>"},{"location":"tug_demo/#instructions-to-start-the-demo_1","title":"Instructions to start the demo","text":"<p>Important</p> <p>If you wish to simulate the speech interaction, be sure to run <code>node-button.js</code> on the machine you used to configure the wifi button, as explained here. This script is in charge of getting the <code>POST</code> request from the button and sending a trigger to <code>managerTUG</code>.</p> <p>The TUG scenario appears within the simulation environment, including the robot, a human model standing in front of it, a chair and two ArUco lines:</p> <p></p> <p>Info</p> <p>The loaded scenario is described in <code>tug-scenario.world</code>.</p> <p>Tip</p> <p>By default, <code>gazebo</code> has the origin and the external gui visible. To remove the origin, you can click on <code>View</code> and deselect <code>Origin</code>. To remove the gui, you can click on <code>Window</code> and <code>Full screen</code>.</p> <p>When the demo is launched, <code>managerTUG</code> waits for the command <code>start</code> to run the TUG session, which can be sent to the rpc port <code>/managerTUG/cmd:rpc</code>. Once this is provided, the interaction starts and the simulated user successfully completes the test, while its step length is computed in real-time. The following video shows a successful test:</p> <p></p> <p>Tip</p> <p>The demo keeps going on even after the TUG is completed, with the robot engaging the human model, which executes the TUG.</p> <p>If you want to stop the demo, you will need to provide a <code>stop</code> command to  the rpc port <code>/managerTUG/cmd:rpc</code>.</p>"},{"location":"tug_demo/#simulating-the-verbal-interaction","title":"Simulating the verbal interaction","text":"<p>The verbal interaction between the robot and the human model is simulated through the following steps:</p> <ul> <li>the Mystrom wifi button is integrated within the simulated demo: when the button is pressed, the human model / the robot (depending which one is moving) stops and a question can be \"asked\";</li> <li>a question is simulated through the <code>question-simulator.sh</code> script. The user can choose 4 keywords, specifically <code>speed</code>, <code>repetition</code>, <code>aid</code>, <code>feedback</code>, each associated to a set of questions. The script randomly selects a question associated to the chosen keyword and sends it to <code>googleSpeechProcess</code> which analyses the sentence; when <code>googleSpeechProcess</code> provides the output to <code>managerTUG</code>, the human model / the robot starts walking / navigating from where it stopped.</li> </ul> <p>The following video shows how the verbal interaction is integrated within the demo:</p> <p></p>"},{"location":"tug_demo/#simulating-failures","title":"Simulating failures","text":"<p>Thrift services are provided in order to simulate the human model failing the TUG. Two possible failures are implemented:</p> <ul> <li>the human model does not reach the target: the robot says that the test was not passed. This can be achieved providing the command <code>set_target</code> to <code>/managerTUG/cmd:rpc</code>, as shown in the following video:</li> </ul> <p></p> <p>Note</p> <p>The target is defined with respect to the gazebo world frame, thus <code>X</code> pointing forward (with respect to the human model), <code>Y</code> pointing left, <code>Z</code> pointing up.</p> <ul> <li>the human model stops:  the robot encourages the user to finish the test, reminding to push the button to ask questions. If the human model does not complete the test, the test is not passed. This can be achieved providing the command <code>pause</code> to <code>/tug_input_port</code>, as shown in the following video:</li> </ul> <p></p> <p>Tip</p> <p>You can also set the time during which the human model has to be paused, by specifying the seconds after <code>pause</code>. For example, to pause it for <code>2 s</code>, you can provide <code>pause 2.0</code> to <code>tug_input_port</code>: after the <code>2 s</code>, the human model starts walking again.</p>"},{"location":"tug_lines/","title":"How to detect TUG start and finish lines","text":"<p>This tutorial will show you how to detect start and finish lines defined for the TUG on the robot and in <code>gazebo</code>.</p>"},{"location":"tug_lines/#using-aruco-boards","title":"Using ArUco boards","text":"<p>In order to identify a line, we rely on ArUco boards provided by <code>OpenCV</code>, which are sets of markers whose relative position is known a-priori. <code>OpenCV</code> provides the API to easily create and detect such boards.</p> <p>Tip</p> <p>A board compared to a set of independent markers has the advantage of providing an estimated pose usually more accurate, which can be computed even in presence of occlusions or partial views (not all markers are required to perform pose estimation).</p> <p>For the TUG, we define two lines, which are ArUco boards with a single line of 6 markers, belonging to two different marker dictionaries and thus uniquely identified.</p> <p>Specifically:</p> <ul> <li>the <code>start-line</code>: where the user's chair is placed;</li> <li> <p>the <code>finish-line</code>: which the user has to cross;</p> <p>Info</p> <p>Check out more on markers and dictionaries here.</p> </li> </ul> <p>The lines we use can be found here. Each line is composed of two images, ending with <code>_0</code> and <code>_1</code>. You will need to print them separately and then stick them together as following:</p> <code>start-line</code> <p> </p> <code>finish-line</code> <p> </p> origin in the bottom left corner  <code>x</code> (red) pointing along the line's length  <code>y</code> (green) pointing along the width  <code>z</code> (blue) pointing out of the line <p>Hint</p> <p>This is required to have a board big enough to be robustly detected in the image, given we use a camera resolution of <code>320x240 px</code>.</p>"},{"location":"tug_lines/#rationale-a-world-made-of-lines-robots-and-skeletons","title":"Rationale: a world made of lines, robots and skeletons","text":"<p>Why do we need such lines?</p> <p>The TUG itself requires a finish line indicating the end of the path to cross before going back to the chair. We decided to rely on robot's perception and visually identify such line only once before starting the exercise. Such information feeds our database and thus the information is kept even if / when the line is not visible.  </p> <p>When the odometry starts, the robot is assumed to be in the origin, which is however a random point in the world. In order to have a starting reference point for the odometry, we further introduced an additional line, the <code>start-line</code>: when such line is detected, the line frame becomes the world, with <code>x</code> pointing along the line length, <code>y</code> along its width and <code>z</code> out of the line, and robot and skeletons are referred to it.</p> <p>The following shows an example of our world:</p> <p> </p>"},{"location":"tug_lines/#detecting-lines-on-the-robot","title":"Detecting lines on the robot","text":"<p>Assumptions</p> <p>We assume the robot is running, with the motors on.</p> <p>On the robot <code>assistive-rehab</code> and its dependencies are already set, with the required applications in the <code>yarpmanager</code>. However, you can check requirements and installation instructions here.</p> <p>We will need to move the robot around and for doing so we can use the joystick to control it. Turn on the joystick, open <code>yarpmanager</code> and click <code>R1_joystick_mobile_base_control</code>. Hit run and connect.</p> <p>Info</p> <p>The required app can be found here.</p> <p>Alternatively</p> <p>Open <code>yarpmotorgui --robot cer --parts \"(mobile_base)\"</code> and click on the yellow pause icon to idle the wheels. Now you will be able to manually move the robot around.</p> <p>In the <code>yarpmanager</code>, click on <code>Assistive Rehabilitation lineDetector App</code> , hit run and connect.</p> <p>Note</p> <p>If you are already running the demo, you can skip this last step: all the required modules are running already.</p> <p>Info</p> <p><code>navController</code> and <code>cer_gaze_controller</code> are required from <code>lineDetector</code> to estimate the robot position with respect to the start line.</p> <p>Next step is to adjust the robot's head to have the lines in the field of view. Therefore, open a terminal and type:</p> <pre><code>yarpmotorgui --robot cer --parts \"(head)\"\n</code></pre> <p>This will open the GUI where you will be able to directly control the head of the robot. Move the first joint (pitch) until the line is visible in <code>yarpview</code> (up to ~<code>20 degree</code>).</p> <p>Now use the joystick to move the robot in front of the <code>start-line</code>.</p> <p>Mind the order</p> <p>The order of detection is important: since the <code>start-line</code> is used as reference frame, this has to be detected first.</p> <p>Open a terminal and type:</p> <p>````tab=\"Terminal 1\" yarp rpc /lineDetector/cmd:rpc detect start-line <pre><code>&lt;p align=\"center\"&gt; &lt;img src=\"https://user-images.githubusercontent.com/9716288/89505153-7506ce00-d7c9-11ea-8b36-30582220810f.gif\" width=700&gt; &lt;/p&gt;\n\nBefore starting the detection, the robot is in a random position:\n\n- when the detection starts, the estimated pose appears on the `yarp` viewer;\n- when the `ack` is provided, the detection is complete: the pose is estimated and the line appears on the `skeletonViewer`, with the robot position recalculated with respect to the line.\nFrom now on, the start line is the origin of the world and robot and skeletons positions are referred to it.  \n\n!!! note\n    The estimated pose is filtered by means of a median filter, thus the detection takes few seconds.\n\nWe need now to detect the `finish-line`. As before, move the robot in front of the line and in `Terminal 1`, type:\n\n````tab=\"Terminal 1\"\ndetect finish-line\n</code></pre></p> <p>Success</p> <p>When the command provides an <code>ack</code>, the detection is complete and the finish line appears on <code>skeletonViewer</code>.</p> <p><p> </p></p> <p>Important</p> <p>The robot navigates the environment, reaching the desired targets with errors accepted within some tolerances (defined in <code>navController</code>). Such errors accumulate over time and after a while it might be necessary to update the robot position (the real robot position drifts from what you see on <code>skeletonViewer</code>).</p> <p>For doing so, you can physically move the robot to one of the lines and run the command <code>update_odometry</code>, specifying the line's tag and the robot orientation with respect to the <code>start-line</code>: <code>skeletonViewer</code> will show the updated position.</p>"},{"location":"tug_lines/#detecting-lines-in-gazebo","title":"Detecting lines in <code>gazebo</code>","text":"<p>Now we are going to see how to detect these lines in <code>gazebo</code>.</p>"},{"location":"tug_lines/#dependencies","title":"Dependencies","text":"<p>After installing <code>assistive-rehab</code> and its requirements, you will need the following dependencies:</p> <ul> <li>gazebo: for running the virtual environment;</li> <li>gazebo-yarp-plugins: for exposing <code>YARP</code> interfaces in <code>gazebo</code>;</li> <li>cer-sim: which includes the robot model loaded by <code>gazebo</code> in <code>AssistiveRehab-TUG_SIM.xml.template</code>;</li> <li>cer: for running the <code>cer_gaze-controller</code>;</li> <li>navigation: for running <code>baseControl</code>.</li> </ul>"},{"location":"tug_lines/#preparing-your-environment","title":"Preparing your environment","text":"<p>The first step you need to take is to prepare your environment.</p> <p>The folder <code>assistive-rehab/app/gazebo/tug</code> includes the <code>sdf</code> models of the Aruco boards (namely <code>aruco_board_start</code> and <code>aruco_board_finish</code>) and the world including the robot and the lines (namely <code>tug-scenario.world</code>).</p> <p>Following this gazebo tutorial, you need to let <code>gazebo</code> know where models and worlds are located. For doing this, you will need to set the following environment variables:</p> <ul> <li><code>GAZEBO_MODEL_PATH</code>: has to point to the folder including the models (<code>assistive-rehab/app/gazebo/tug</code>);</li> <li><code>GAZEBO_RESOURCE_PATH</code>: has to point to the folder including the world application (<code>assistive-rehab/app/gazebo/tug</code>);</li> </ul>"},{"location":"tug_lines/#running-the-application","title":"Running the application","text":"<p>In <code>assistive-rehab/app/scripts</code> you will find the application <code>AssistiveRehab-TUG_SIM.xml.template</code> to run the TUG demo in <code>gazebo</code>.</p> <p>First, you need to run <code>yarpserver</code>, opening a terminal and typing:</p> <pre><code>yarpserver\n</code></pre> <p>Open <code>yarpmanager</code>, run the <code>AssistiveRehab-TUG_SIM.xml</code> and hit connect.</p> <p>Warning</p> <p>The <code>xml</code> is conceived for the complete TUG demo, therefore not all modules are required for this specific application. In the complete application, <code>lineDetector</code> takes the RGB input propagated from <code>yarpOpenPose</code>. For this specific application, you don't need to run <code>yarpOpenPose</code> and can simply connect <code>/SIM_CER_ROBOT/depthCamera/rgbImage:o</code> to <code>/lineDetector/img:i</code>.</p> <p>You will need to run the following modules:</p> <ul> <li><code>gazebo</code>: to run <code>gazebo</code> and load the world;</li> <li><code>objectsPropertiesCollector</code>: to store the robot skeleton and the lines within a yarp-oriented database;</li> <li><code>robotSkeletonPublisher</code>: to publish the skeleton representing the robot;</li> <li><code>baseControl</code>: to control the robot's base;</li> <li><code>navController</code>: to send commands to <code>baseControl</code>;</li> <li><code>cer_gaze-controller</code>: to control the robot's gaze;</li> <li><code>lineDetector</code>: to detect the lines;</li> <li><code>yarpview --name /viewer/line</code>: to visualize the detected lines in the RGB image;</li> <li><code>skeletonViewer</code>: to visualize the detected lines and the robot skeleton in the world.</li> </ul> <p>Tip</p> <p>You can customize the app by removing unnecessary modules. You can save the app in your folder <code>$HOME/.local/share/yarp</code>and exploit the shadowing mechanism. In this way, when you open <code>yarpmanager</code>, your customized app will be automatically loaded.</p> <p>You should now see the <code>gazebo</code> GUI with the robot and the two Aruco boards loaded, as following:</p> <p></p> <p>You can see there are two Aruco boards: we identify the closer to the robot as <code>finish-line</code> and the further as <code>start-line</code>. Let's detect these lines! You will need to move the robot's head to have the lines in the field of view. Therefore, open a terminal and type:</p> <pre><code>yarpmotorgui --robot SIM_CER_ROBOT --parts \"(head)\"\n</code></pre> <p>As with the real robot, move the first joint (pitch) until the line is visible in <code>yarpview</code> (up to ~<code>20 degree</code>).</p> <p>Since we don't have a joystick to control the robot in <code>gazebo</code>, we can rely on <code>navController</code> for moving it. You will then need to send commands to <code>navController</code> (<code>Terminal 1</code>) for moving the robot and <code>lineDetector</code> (<code>Terminal 2</code>) for detecting the lines. Therefore, open two terminals and type:</p> <p>````tab=\"Terminal 1\" yarp rpc /navController/rpc <pre><code>````tab=\"Terminal 2\"\nyarp rpc /lineDetector/cmd:rpc\n</code></pre></p> <p>Note</p> <p>From now on, I will refer to <code>Terminal 1</code> for commands provided to <code>navController</code> and <code>Terminal 2</code> for commands to <code>lineDetector</code>.</p> <p>Let's start to detect the <code>start-line</code>. When the application starts, the start line is out of the robot's field of view. Therefore you have to move the robot to a point of the world where the line is visible. For doing so, you can use the <code>go_to</code> service provided by <code>navController</code>, typing the command shown in <code>Terminal 1</code>. This will move the robot <code>2.5 meters</code> forward keeping its current orientation and you will see the robot approaching the line. When the robot stops, the line will be in the robot's field of view and you can detect it through the <code>detect</code> service provided by <code>lineDetector</code>, typing the command shown in <code>Terminal 2</code>:</p> <p>Note</p> <p>When the application starts, the robot reference frame follows the following convention: <code>X</code> pointing forward, <code>Y</code> pointing left, <code>Z</code> pointing up.</p> <p>````tab=\"Terminal 1 (navController)\" go_to 2.5 0.0 0.0 <pre><code>````tab=\"Terminal 2 (lineDetector)\"\ndetect start-line\n</code></pre></p> <p>Tip</p> <p><code>navController</code> and <code>lineDetector</code> provide services through <code>thrift</code>. Therefore, you can check what's required by the specific service typing <code>help</code> followed by the name of the service (for example <code>help go_to</code>).</p> <p>This is what you should see:</p> <p></p> <p>As with the real robot, when the start line is estimated it appears in the <code>skeletonViewer</code> and becomes the origin of the world: the robot position in the <code>skeletonViewer</code> is recalculated with respect to the line.</p> <p>Let's now detect the <code>finish-line</code>. The procedure is the same as before, therefore you will have to move to robot to a point of the world where the finish line is visible.</p> <p>Warning</p> <p>Keep in mind that the robot position is now referred to the start line. Therefore the robot reference frame is: <code>X</code> pointing right, <code>Y</code> pointing forward, <code>Z</code> pointing up.</p> <p>Again, use the <code>go_to</code> service provided by <code>navController</code>, typing the command shown in <code>Terminal 1</code>. The robot will start approaching the line. When the robot stops, you can detect the finish line through the <code>detect</code> service provided by <code>lineDetector</code>, typing the command shown in <code>Terminal 2</code>:</p> <p>````tab=\"Terminal 1 (navController)\" go_to 0.0 -5.5 90.0 true <pre><code>````tab=\"Terminal 2 (lineDetector)\"\ndetect finish-line\n</code></pre></p> <p>Tip</p> <p>The <code>true</code> flag in <code>Terminal 1</code> allows the robot to move backward.</p> <p>This is what you should see:</p> <p></p> <p>Finally, to check that the line is estimated correctly, you can use the <code>go_to_line</code> service provided by <code>lineDetector</code>, as shown in <code>Terminal 2</code>:</p> <p><code>tab=\"Terminal 2 (lineDetector)\" go_to_line finish-line</code></p> <p>You will see the robot going to the finish line origin:</p> <p></p>"},{"location":"tutorial_intro/","title":"Getting started","text":"<p>If you want to learn how to exploit our framework to create something similar for your own applications, you can start from here:</p> <ul> <li>How to manage a skeleton object</li> <li>How to replay an experiment</li> <li>How to run the visual pipeline in a disembodied manner</li> <li>How to temporally align two signals</li> <li>How to run the rehabilitation demos</li> <li>How to run the virtual demo</li> <li>How to detect TUG start and finish lines</li> <li>How to run the TUG demo</li> <li>How to use the gazebo plugin</li> <li>How to configure the wifi button</li> </ul>"},{"location":"virtual_demo/","title":"How to run the virtual demo","text":"<p>This tutorial will show you how to run the virtual demo. The virtual demo replicates the demo with the real R1 in a virtual environment (<code>Gazebo</code>), with the virtual version of the robot:</p> <p></p> <p>The virtual R1 is shown on a screen with a RealSense on the top (indicated by the red arrow in the picture). In this demo, the user in the field of view is automatically engaged and the interaction includes three phases:</p> <ol> <li>observation phase: the virtual robot welcomes the user and shows the exercise to perform;</li> <li>direct imitation phase: the virtual robot performs the exercise together with the user, while providing a verbal feedback on how the exercise is being performed;</li> <li>occluded imitation phase: the virtual robot keeps performing the exercise behind a panel and stops providing the verbal feedback.</li> </ol> <p>Features like facial expressions, gazing the user and a verbal feedback are also included, such that the interaction is as close as possible to the real one.</p> <p>The related application can be found here, named AssistiveRehab-TWM-virtual.xml.template.</p> <p>The Train with Me study</p> <p>This scenario was adopted to compare the users' engagement and movement performance when exercising with a real robot and its virtual counterpart. Results show that both the levels of engagement and the participants\u2019 performance are higher with the real robot than with the virtual one! Check it out in our paper.</p>"},{"location":"virtual_demo/#dependencies","title":"Dependencies","text":"<p>After installing <code>assistive-rehab</code>, you will need the following dependencies:</p> <ul> <li>RealSense: for running the RealSense;</li> <li>cer: for running the gaze-controller and face expressions;</li> <li>gazebo: for running the virtual environment;</li> <li>gazebo-yarp-plugins: for exposing YARP interfaces in <code>Gazebo</code>;</li> <li>cer-sim: which includes the model loaded by <code>Gazebo</code> in AssistiveRehab-TWM-virtual.xml.template;</li> <li>speech: for running the <code>iSpeak</code> module.</li> </ul>"},{"location":"virtual_demo/#requirements","title":"Requirements","text":"<p>The following hardware is required:</p> <ul> <li>RealSense camera;</li> <li>NVIDIA graphics card: for running <code>yarpOpenPose</code> and <code>actionRecognizer</code>.</li> </ul>"},{"location":"virtual_demo/#run-the-virtual-demo","title":"Run the virtual demo","text":"<p>To run the demo, first run <code>yarpserver</code>. Connect the RealSense to your laptop. Open <code>yarpmanager</code>, run the <code>AssistiveRehab-TWM-virtual App</code> and connect. A virtual R1 appears within the simulation environment.</p> <p>Note</p> <p>By default, <code>Gazebo</code> has the origin and the external gui visible. To remove the origin, you can click on <code>View</code> and deselect <code>Origin</code>. To remove the gui, you can click on <code>Window</code> and <code>Full screen</code>.</p> <p>When the demo is launched, the <code>interactionManager</code> waits for the command <code>start_observation</code> to start the exercise session. Specifically, the following commands should be sent in this order to the rpc port <code>/interactionManager/cmd:rpc</code>:</p> <ul> <li><code>start_observation</code>: to start the observation phase, where the robot looks for a user and, when it founds her/him, starts showing the exercise to be performed;</li> <li><code>start_imitation</code>: to start the direct imitation phase, where the robot performs the exercise together with the user, while providing her/him with verbal feedback;</li> <li><code>start_occlusion</code>: to start the occluded imitation phase, where the robot keeps performing the exercise behind a panel and stops providing the verbal feedback;</li> <li><code>stop</code>: to stop the interaction.</li> </ul> <p>Tip</p> <p>A different kind of interaction is also allowed, requiring the user to raise her/his hand to start the interaction. Such interaction includes the observation and the direct imitation phases, with the virtual robot showing the exercise to perform and, right after, performing the exercise while providing a verbal feedback (this is the virtual version of the demo Y1M5). In this configuration, the command <code>start_with_hand</code> starts the interaction and, when the user raises her/his hand, the two phases are run in a row, one after the other, without waiting any additional command.</p> <p>The following picture shows an example of interaction during the direct and occluded imitation phase:</p> <p></p> <p>During the direct imitation phase, the robot moves while providing verbal feedback (the mouth appears on the robot's screen). During the occluded imitation phase, a panel appears occluding the moving arm and the robot stops providing verbal feedback (the mouth only appears at the end of the exercise, when the robot warns the user that the exercise is over).</p> <p>Note</p> <p>The shown interaction is just an example and the number of repetitions of the exercise is higher by default.</p> <p>The parameters used for this application can be found in the context <code>train-with-me</code>. Specifically, parameters that control the repetitions of the arm movements are defined in the  <code>interactionManager.ini</code> as <code>nrep-show</code> and <code>nrep-perform</code>, respectively for the observation phase (set to <code>8</code> by default) and the imitation phase, both direct and occluded (set to <code>16</code> by default). The command <code>start_occlusion</code> can be sent after the eighth  repetition of the movement within the direct phase, to have <code>8</code> repetitions for each phase.</p>"},{"location":"wifi_button/","title":"How to configure the wifi button","text":"<p>This tutorial will show you how to configure the Mystrom wifi button.</p> <p>Important</p> <p>This is required if you want to run the TUG demo with the speech interaction.</p>"},{"location":"wifi_button/#using-the-app","title":"Using the app","text":"<p>First, you will need to download the app and create an account.</p> <p>Open the app, click on the symbol in the right corner and click on Add device / WiFi Button.</p> <p>Now we will connect the button to your wifi network. Click on Not connected and select Light. Choose the manual configuration. Go on until this screen appers:</p> <p> </p> <p>Press the button for 1 second and the button will start blinking white.</p> <p>Go on, this screen will appear:</p> <p> </p> <p>Keep the button pressed for 2 seconds, the LED will start blinking red and white: now the button will be in Access Point for 5 minutes.</p> <p>Be patient</p> <p>This might take few seconds.</p> <p>Go on and select and connect to <code>my-button-XXXXXX</code> network, where <code>XXXXXX</code> will change depending on the button.</p> <p>Go back to the app and wait until a list of wifi networks appears.</p> <p>Choose yours and click on Use fixed IP. Now choose an IP address for your button and insert your subnet mask, gateway and DNS. If required insert the password of your network.</p> <p>How to set the IP address</p> <p>The IP address you choose must be in the same subnet of the machine you will interface to. Thus the first three numbers of the IP address you choose should match those of your machine. Follow this guide to know the IP address of your machine.</p> <p>After few seconds the wifi button is found and connected to your network! Go on and choose a name for it and an icon. Done? Congratulations, now your button configured!!</p> <p>You can skip the action setting as I will show you how so set actions in next section.</p>"},{"location":"wifi_button/#setting-the-actions","title":"Setting the actions","text":"<p>This is a guide that shows you how to communicate with your device once it has been configured!</p> <p>To get the device specific information, you can do:</p> <pre><code>curl --location --request GET '[Button IP]/api/v1/device'\n</code></pre> <p>Tip</p> <p>Replace <code>[Button IP]</code> with the one you chose when configuring the button.</p> <p>The output will be a <code>json</code> object with the mac address (without delimiters) and its field, as shown here.</p> <p>Now we want to set the button such that it sends a <code>POST</code> request to our IP if pressed once, which corresponds to the <code>single</code> action.</p> <p>Additional actions</p> <p>For the TUG demo, we only use the <code>single</code> action. As explained here, this button also allows you to set <code>double</code> (http request executed when pressing the button twice) and <code>long</code> (http request executed when pressing the button long).</p> <p>Not familiar with http requests?</p> <p>Check this out!</p> <p>For doing so, you can do:</p> <pre><code>curl -v -d \"single=post://[Your IP]/api/mystrom?key%3Dspeech\" http://[Button IP]/api/v1/device/[Button MAC]\n</code></pre> <p>where:</p> <ul> <li>you need to replace <code>[Your IP]</code> with the IP address of the machine where you want to receive the <code>POST</code> request;</li> <li>you need to replace <code>[Button IP]</code> with the IP address of your button;</li> <li>you need to replace <code>[Button MAC]</code> with the MAC address of your button;</li> <li>the string <code>speech</code> after <code>key%3D</code> is the value of the keyword sent with the <code>POST</code> request.</li> </ul> <p>Button MAC</p> <p>You can find the MAC address of your button in the app: click on the button, go on Settings / Technical specifications. Note that in the <code>curl</code> command, you need to insert the MAC address with no semicolons.</p> <p>Connection refused</p> <p>If you get the following error: Failed to connect to [Button IP] port 80: No route to host, it means that the device is into sleep mode. This is done in order to preserve battery life (the device is visible in the network only after adding to wifi).</p> <p>Luckily, you can enter into maintenance mode, by keeping the button pressed for 5 seconds (it starts blinking green). Now you should be able to communicate again with the device!</p> <p>In this way, we set the <code>single</code> action, which corresponds to the http request executed when pressing the button once.</p> <p>Important</p> <p>If you want to use the wifi button to run the TUG demo, configure the <code>single</code> action to send a <code>POST</code> request to the machine where you run <code>node-button.js</code>.</p> <p>Tip</p> <p>To set <code>double</code> or <code>long</code> action, you can follow the same procedure and replace in the <code>curl</code> command <code>single</code> with the action you want to configure.</p> <p>Now if you get the device specific information, using the command:</p> <pre><code>curl --location --request GET '[Button IP]/api/v1/device'\n</code></pre> <p>you will see a <code>json</code> output:</p> <pre><code>{\n    \"[Button MAC]\":\n    {\"type\": \"button\", \"battery\": true, \"reachable\": true, \"meshroot\": false,\n      \"charge\": false, \"voltage\": 4.179, \"fw_version\": \"2.74.31\",\n      \"single\": \"post:\\/\\/[Your IP]]\\/api\\/mystrom?key=speech\",\n      \"double\": \"\", \"long\": \"\", \"touch\": \"\", \"generic\": \"\",\n      \"connectionStatus\": {\"ntp\": true, \"dns\": true, \"connection\": true,\n                          \"handshake\": true, \"login\": true}, \"name\": \"\"}\n}\n</code></pre> <p>where the <code>single</code> action is configured as <code>\"post:\\/\\/[Your IP]]\\/api\\/mystrom?key=speech\"</code>.</p>"}]}